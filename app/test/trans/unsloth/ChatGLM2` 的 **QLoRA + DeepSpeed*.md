å½“ç„¶å¯ä»¥ï¼ä¸‹é¢æ˜¯åŸºäº `ChatGLM2` çš„ **QLoRA + DeepSpeed** ä¸­æ–‡å¾®è°ƒçš„å®Œæ•´ä»£ç ç¤ºä¾‹ï¼ˆä½¿ç”¨ HuggingFace + Transformers + peft + bitsandbytes + DeepSpeedï¼‰ã€‚å¯åœ¨æ¶ˆè´¹çº§ GPU ä¸Šè¿›è¡Œé«˜æ•ˆå¾®è°ƒã€‚

---

## âœ… æ€»è§ˆ

| æ¨¡å— | è¯´æ˜ |
|------|------|
| æ¨¡å‹ | `ChatGLM2-6B`ï¼ˆä¸­æ–‡å¤§æ¨¡å‹ï¼‰ |
| å¾®è°ƒæ–¹æ³• | `QLoRA`ï¼ˆ4bit é‡åŒ– + LoRAï¼‰ |
| åŠ é€Ÿæ¡†æ¶ | `DeepSpeed`ï¼ˆZeRO Stage 2/3ï¼‰ |
| æ•°æ®é›† | ä¸­æ–‡æŒ‡ä»¤å¼å¾®è°ƒæ•°æ®é›†ï¼ˆå¦‚ Alpaca æ ¼å¼ï¼‰ |

---

## ğŸ§± 1. å®‰è£…ä¾èµ–

```bash
pip install torch transformers datasets accelerate bitsandbytes peft deepspeed
```

---

## ğŸ“¦ 2. DeepSpeed é…ç½®æ–‡ä»¶ï¼ˆ`ds_config_zero2.json`ï¼‰

```json
{
  "train_batch_size": 8,
  "train_micro_batch_size_per_gpu": 1,
  "gradient_accumulation_steps": 8,
  "gradient_clipping": 1.0,
  "fp16": {
    "enabled": true
  },
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "cpu"
    }
  }
}
```

---

## ğŸ§  3. å¾®è°ƒè„šæœ¬ï¼š`train_chatglm2_qlora_deepspeed.py`

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model
from datasets import load_dataset

# 1. åŠ è½½ Tokenizer å’Œ 4bit é‡åŒ–æ¨¡å‹
model_name = "THUDM/chatglm2-6b"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,
    device_map="auto",
    trust_remote_code=True
)

# 2. LoRA é…ç½®ï¼ˆQLoRAï¼‰
model = prepare_model_for_kbit_training(model)
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["query_key_value"],  # ChatGLM çš„ LoRA æ’å…¥ç‚¹
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)

# 3. åŠ è½½ Alpaca æ ¼å¼ä¸­æ–‡æ•°æ®
# æ ¼å¼ï¼š[{"instruction": ..., "input": ..., "output": ...}]
dataset = load_dataset("json", data_files="data/alpaca_zh.json", split="train")

def preprocess(example):
    prompt = f"### é—®é¢˜ï¼š\n{example['instruction']}\n{example['input']}\n\n### å›ç­”ï¼š\n"
    input_ids = tokenizer(prompt, truncation=True, max_length=512, padding="max_length")
    label_ids = tokenizer(example["output"], truncation=True, max_length=512, padding="max_length")["input_ids"]
    input_ids["labels"] = label_ids
    return input_ids

dataset = dataset.map(preprocess)

# 4. TrainingArguments å¯ç”¨ DeepSpeed
training_args = TrainingArguments(
    output_dir="chatglm2_qlora_ds_output",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    save_strategy="epoch",
    deepspeed="ds_config_zero2.json",
    remove_unused_columns=False,
    report_to="none",
)

# 5. Trainer å¯åŠ¨è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer,
    data_collator=DataCollatorForSeq2Seq(tokenizer, model),
)

trainer.train()
```

---

## ğŸš€ 4. å¯åŠ¨è®­ç»ƒ

```bash
deepspeed train_chatglm2_qlora_deepspeed.py
```

---

## ğŸ§ª 5. æ¨ç†ç¤ºä¾‹

```python
model.eval()
prompt = "### é—®é¢˜ï¼š\nè¯·å°†ä»¥ä¸‹å¥å­ç¿»è¯‘æˆè‹±æ–‡ã€‚\nä½ å¥½ï¼Œæ¬¢è¿æ¥åˆ°åŒ—äº¬ã€‚\n\n### å›ç­”ï¼š\n"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

---

## ğŸ 6. æ€»ç»“

| æ¨¡å— | å†…å®¹ |
|------|------|
| æ¨¡å‹ | ChatGLM2-6B |
| å¾®è°ƒ | QLoRAï¼ˆ4bité‡åŒ– + LoRAï¼‰ |
| æ¡†æ¶ | HuggingFace + DeepSpeed |
| ä¼˜åŠ¿ | æ˜¾å­˜éœ€æ±‚ä½ï¼Œæ”¯æŒä¸­å¤§å‹ä¸­æ–‡ä»»åŠ¡ |

---

éœ€è¦æˆ‘æ¥ç€ç»™å‡º **ZeRO Stage 3 + å¤šæœºå¤šå¡è®­ç»ƒç‰ˆæœ¬**ï¼Œæˆ–æ”¯æŒ **wandb ç›‘æ§ + PEFT æ¨¡å‹å¯¼å‡º** å—ï¼Ÿ