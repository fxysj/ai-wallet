å¥½çš„ï¼ä¸‹é¢æ˜¯ä¸€ä¸ª **åŸºäº Unsloth + DeepSpeed + LLaMA3/ChatGLM2 + LoRA çš„ä¸­æ–‡å¤§æ¨¡å‹å¾®è°ƒå®Œæ•´ç¤ºä¾‹**ï¼Œæ”¯æŒåœ¨æ¶ˆè´¹çº§ GPU ä¸Šè¿›è¡Œä½èµ„æºå¾®è°ƒï¼Œå…·å¤‡ä»¥ä¸‹ä¼˜åŠ¿ï¼š

- ä½¿ç”¨ **Unsloth** é«˜æ•ˆé€‚é… LoRA / QLoRA
- ä½¿ç”¨ **DeepSpeed ZeRO Stage 2/3** å®ç°å†…å­˜ä¼˜åŒ–
- æ”¯æŒä¸­æ–‡ä»»åŠ¡ï¼ˆæƒ…æ„Ÿåˆ†ç±»ã€é—®ç­”ã€æ‘˜è¦ç­‰ï¼‰

---

## ğŸ§© ä¸€ã€ç¯å¢ƒå‡†å¤‡

æ¨è Python 3.10 + CUDA 11.8/12.1ï¼ˆæ”¯æŒ Ampere æˆ–æ›´æ–° GPUï¼‰

```bash
conda create -n unsloth_ds python=3.10 -y
conda activate unsloth_ds

pip install "unsloth[deepspeed] @ git+https://github.com/unslothai/unsloth.git"
pip install peft accelerate bitsandbytes deepspeed
```

---

## ğŸ§  äºŒã€åŠ è½½ LLaMA3/ChatGLM2 æ¨¡å‹ + Unsloth

> ä¸‹é¢ä»¥ LLaMA3-8B ä¸ºä¾‹ï¼ˆChatGLM2 ä½¿ç”¨ HuggingFace æ ¼å¼ä¹Ÿå¯ç”¨ï¼‰

```python
from unsloth import FastLanguageModel
import torch

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Meta-Llama-3.1-8B",  # æˆ– chatglm2-hf ç‰ˆæœ¬
    max_seq_length=2048,
    dtype=torch.float16,
    load_in_4bit=True,  # QLoRA
)
```

---

## ğŸ§° ä¸‰ã€LoRA é…ç½®

```python
from peft import LoraConfig, get_peft_model, TaskType

lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
)

model = get_peft_model(model, lora_config)
```

---

## ğŸ“š å››ã€å‡†å¤‡ä¸­æ–‡æ•°æ®é›†ï¼ˆè‡ªå®šä¹‰æ ¼å¼ï¼‰

æ ¼å¼å¦‚ä¸‹ï¼ˆinstruction tuningï¼‰ï¼š

```json
[
  {
    "instruction": "è¯·å°†ä¸‹é¢çš„å¥å­ç¿»è¯‘æˆè‹±æ–‡ï¼š",
    "input": "ä½ å¥½ï¼Œæ¬¢è¿æ¥åˆ°åŒ—äº¬ã€‚",
    "output": "Hello, welcome to Beijing."
  }
]
```

åŠ è½½å’Œé¢„å¤„ç†ï¼š

```python
from datasets import load_dataset

dataset = load_dataset("json", data_files="data.json", split="train")

def preprocess(example):
    prompt = example["instruction"] + "\n" + example["input"]
    input_ids = tokenizer(prompt, truncation=True, padding="max_length", max_length=512)
    label_ids = tokenizer(example["output"], truncation=True, padding="max_length", max_length=512)["input_ids"]
    input_ids["labels"] = label_ids
    return input_ids

dataset = dataset.map(preprocess)
```

---

## âš™ï¸ äº”ã€DeepSpeed é…ç½®æ–‡ä»¶ï¼ˆä¿å­˜ä¸º `ds_config.json`ï¼‰

```json
{
  "train_batch_size": 4,
  "gradient_accumulation_steps": 4,
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": 2e-4,
      "betas": [0.9, 0.999],
      "eps": 1e-8,
      "weight_decay": 0.01
    }
  },
  "fp16": {
    "enabled": true
  },
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "cpu"
    },
    "allgather_partitions": true,
    "allgather_bucket_size": 2e8,
    "overlap_comm": true,
    "reduce_scatter": true,
    "reduce_bucket_size": 2e8
  },
  "gradient_clipping": 1.0
}
```

---

## ğŸ‹ï¸ å…­ã€å¯åŠ¨å¾®è°ƒè®­ç»ƒ

```python
from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq

training_args = TrainingArguments(
    output_dir="./output_unsloth_deepspeed",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    save_strategy="epoch",
    remove_unused_columns=False,
    deepspeed="ds_config.json",  # ğŸ§  å¯ç”¨ DeepSpeedï¼
    report_to="none",  # å¯é€‰ï¼šwandb
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer,
    data_collator=DataCollatorForSeq2Seq(tokenizer, model),
)

trainer.train()
```

---

## ğŸ’¾ ä¸ƒã€ä¿å­˜å¾®è°ƒæ¨¡å‹

```python
model.save_pretrained("output_unsloth_deepspeed")
tokenizer.save_pretrained("output_unsloth_deepspeed")
```

---

## âœ… å…«ã€æ¨ç†ç¤ºä¾‹

```python
prompt = "è¯·å°†ä¸‹é¢çš„å¥å­ç¿»è¯‘æˆè‹±æ–‡ï¼š\nä½ å¥½ï¼Œæ¬¢è¿æ¥åˆ°åŒ—äº¬ã€‚"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

---

## ğŸ”š æ€»ç»“

| æ¨¡å—       | æè¿°                                             |
|------------|--------------------------------------------------|
| `Unsloth`  | é«˜æ•ˆé€‚é… LLaMA3ã€LoRAï¼Œå…¼å®¹ Transformers Trainer |
| `DeepSpeed`| æ”¯æŒ ZeRO2/ZeRO3 åˆ†å¸ƒå¼ä¼˜åŒ–ï¼ŒèŠ‚çœæ˜¾å­˜æå‡åå    |
| æ•°æ®æ ¼å¼   | æ”¯æŒ `instruction + input + output` ä¸­æ–‡ä»»åŠ¡     |
| æ”¯æŒæ¨¡å‹   | LLaMA3ã€ChatGLM2ã€Qwenã€Baichuan ç­‰              |

---

éœ€è¦æˆ‘è¿›ä¸€æ­¥æä¾›ï¼šğŸ‘‰ ChatGLM2 + QLoRA + DeepSpeed çš„å®Œæ•´ä»£ç ï¼Œæˆ– DeepSpeed ZeRO-3 å¤šå¡è®­ç»ƒå¯åŠ¨å‘½ä»¤ï¼Ÿ