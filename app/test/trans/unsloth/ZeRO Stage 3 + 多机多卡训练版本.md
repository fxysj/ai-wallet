ZeRO Stage 3 + å¤šæœºå¤šå¡è®­ç»ƒç‰ˆæœ¬ 
ä¸‹é¢æ˜¯åŸºäº **ChatGLM2 + QLoRA + DeepSpeed ZeRO Stage 3 + å¤šæœºå¤šå¡è®­ç»ƒ** çš„å®Œæ•´ç¤ºä¾‹ä»£ç å’Œé…ç½®ã€‚è¯¥ç‰ˆæœ¬é€‚ç”¨äºè·¨å¤šå°æœåŠ¡å™¨è¿›è¡Œå¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒï¼Œæå¤§é™ä½æ˜¾å­˜å‹åŠ›ï¼Œå¹¶æå‡è®­ç»ƒæ•ˆç‡ã€‚

---

## ğŸ§  æ¨¡å‹ä¸è®­ç»ƒæ–¹æ¡ˆ

| é¡¹ç›® | å†…å®¹ |
|------|------|
| æ¨¡å‹ | `ChatGLM2-6B`ï¼ˆä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹ï¼‰ |
| å¾®è°ƒ | `QLoRA`ï¼ˆLoRA + 4bité‡åŒ–ï¼‰ |
| æ¡†æ¶ | HuggingFace Transformers + PEFT + DeepSpeed |
| åˆ†å¸ƒå¼æ–¹æ¡ˆ | ZeRO Stage 3 + å¤šæœºå¤šå¡ (`deepspeed --hostfile`) |

---

## ğŸ“ ç›®å½•ç»“æ„å»ºè®®

```bash
project/
â”œâ”€â”€ ds_config_zero3.json           # ZeRO Stage 3 é…ç½®æ–‡ä»¶
â”œâ”€â”€ train_chatglm2_zero3.py       # å¯åŠ¨è®­ç»ƒè„šæœ¬
â”œâ”€â”€ data/
â”‚   â””â”€â”€ alpaca_zh.json            # ä¸­æ–‡æ•°æ®é›†
â”œâ”€â”€ hostfile                      # ä¸»æœº IP åˆ—è¡¨
```

---

## ğŸ“¦ 1. DeepSpeed é…ç½®ï¼ˆ`ds_config_zero3.json`ï¼‰

```json
{
  "train_micro_batch_size_per_gpu": 1,
  "gradient_accumulation_steps": 8,
  "gradient_clipping": 1.0,
  "bf16": {
    "enabled": true
  },
  "zero_optimization": {
    "stage": 3,
    "offload_optimizer": {
      "device": "cpu",
      "pin_memory": true
    },
    "offload_param": {
      "device": "cpu",
      "pin_memory": true
    },
    "overlap_comm": true,
    "contiguous_gradients": true,
    "reduce_bucket_size": 5e7,
    "stage3_prefetch_bucket_size": 5e7,
    "stage3_param_persistence_threshold": 1e5
  },
  "activation_checkpointing": {
    "partition_activations": true,
    "contiguous_memory_optimization": true
  },
  "aio": {
    "block_size": 1048576,
    "queue_depth": 8,
    "single_submit": false,
    "overlap_events": true,
    "thread_count": 1
  }
}
```

---

## ğŸ§‘â€ğŸ’» 2. Python å¾®è°ƒè„šæœ¬ï¼ˆ`train_chatglm2_zero3.py`ï¼‰

ï¼ˆä¸ä¹‹å‰ç‰ˆæœ¬å¤§ä½“ä¸€è‡´ï¼Œä»…è¡¥å……å…³é”®è¯´æ˜ï¼‰

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq
from datasets import load_dataset
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model

model_name = "THUDM/chatglm2-6b"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,
    device_map="auto",
    trust_remote_code=True
)

model = prepare_model_for_kbit_training(model)
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["query_key_value"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)

dataset = load_dataset("json", data_files="data/alpaca_zh.json", split="train")

def preprocess(example):
    prompt = f"### é—®é¢˜ï¼š\n{example['instruction']}\n{example['input']}\n\n### å›ç­”ï¼š\n"
    inputs = tokenizer(prompt, truncation=True, padding="max_length", max_length=512)
    labels = tokenizer(example["output"], truncation=True, padding="max_length", max_length=512)
    inputs["labels"] = labels["input_ids"]
    return inputs

dataset = dataset.map(preprocess)

training_args = TrainingArguments(
    output_dir="output_zero3",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
    num_train_epochs=3,
    bf16=True,
    logging_steps=10,
    save_strategy="epoch",
    deepspeed="ds_config_zero3.json",
    remove_unused_columns=False,
    report_to="none",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer,
    data_collator=DataCollatorForSeq2Seq(tokenizer, model),
)

trainer.train()
```

---

## ğŸŒ 3. ä¸»æœºæ–‡ä»¶ï¼ˆ`hostfile`ï¼‰

```text
worker-1 slots=4
worker-2 slots=4
```

å…¶ä¸­ `worker-1` å’Œ `worker-2` æ˜¯ä¸¤å°æœºå™¨çš„ hostname æˆ–å†…ç½‘ IPï¼Œ`slots=4` è¡¨ç¤ºæ¯å°æœºå™¨ä½¿ç”¨ 4 ä¸ª GPUã€‚

---

## ğŸš€ 4. å¯åŠ¨è®­ç»ƒ

```bash
deepspeed --hostfile hostfile train_chatglm2_zero3.py
```

ä½ ä¹Ÿå¯ä»¥æŒ‡å®šèŠ‚ç‚¹æ•° + GPU æ•°è¿è¡Œï¼š

```bash
deepspeed --num_nodes 2 --num_gpus 4 train_chatglm2_zero3.py
```

---

## âœ… 5. ä¼˜åŠ¿æ€»ç»“

| æ¨¡å— | ä¼˜ç‚¹ |
|------|------|
| ZeRO Stage 3 | å‚æ•° & ä¼˜åŒ–å™¨ &æ¢¯åº¦å‡åˆ†å¸ƒå†…å­˜ |
| å¤šæœºå¤šå¡ | è·¨æœåŠ¡å™¨å¹¶è¡Œè®­ç»ƒ |
| QLoRA | æ˜¾å­˜æçœ + ç²¾åº¦é«˜ |
| ChatGLM2 | ä¸­æ–‡èƒ½åŠ›ä¼˜ç§€ï¼Œé€‚ç”¨äºæŒ‡ä»¤å¾®è°ƒ |

---

å¦‚éœ€æ”¯æŒ `wandb` è®­ç»ƒç›‘æ§ï¼Œ`SFTTrainer` ç²¾è°ƒå™¨ã€æˆ–è€…è‡ªåŠ¨ä¿å­˜ `LoRA adapter`ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ ä¸€ä¸€è¡¥å……ã€‚æ˜¯å¦è¿˜æƒ³é›†æˆæ¨ç†éƒ¨ç½²ï¼ˆå¦‚ FastAPI + model.generateï¼‰ï¼Ÿ