ä»¥ä¸‹æ˜¯å®Œæ•´çš„ **ChatGLM3 + QLoRA + WandB** ç²¾è°ƒç¤ºä¾‹ä»£ç ï¼Œå·²è¡¥å……è¯¦ç»†æ³¨é‡Šï¼Œå¸®åŠ©ä½ ç†è§£æ¯ä¸€æ­¥çš„ä½œç”¨ï¼š

---

## âœ… è®­ç»ƒæµç¨‹æ¦‚è§ˆï¼š

| æ­¥éª¤ | å†…å®¹ |
|------|------|
| âœ… æ¨¡å‹åŠ è½½ | ä½¿ç”¨ ChatGLM3 + 4bit QLoRA |
| âœ… æ•°æ®å‡†å¤‡ | ä¸­æ–‡æŒ‡ä»¤æ•°æ®ï¼ˆæ”¯æŒ Alpaca æ ¼å¼ï¼‰ |
| âœ… å¾®è°ƒæ¡†æ¶ | ğŸ¤— Transformers + PEFT + TRL çš„ `SFTTrainer` |
| âœ… æ—¥å¿—ç›‘æ§ | æ”¯æŒ WandB |

---

## âœ… å®Œæ•´ä»£ç ï¼ˆå«æ³¨é‡Šï¼‰ï¼š

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset
from trl import SFTTrainer
import torch
import wandb

# === Step 1: æ¨¡å‹å’Œé‡åŒ–é…ç½® ===
model_name = "THUDM/chatglm3-6b"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,  # ä½¿ç”¨ 4bit é‡åŒ–
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",  # ç²¾åº¦æ›´é«˜çš„éå¯¹ç§°é‡åŒ–
    bnb_4bit_compute_dtype=torch.bfloat16  # ä½¿ç”¨ bfloat16 æå‡æ€§èƒ½
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token  # è®¾ç½® pad_token é˜²æ­¢ padding æŠ¥é”™

# === Step 2: åº”ç”¨ QLoRA é…ç½® ===
model = prepare_model_for_kbit_training(model)

lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["query_key_value"],  # ChatGLM3 å…³é”®æ¨¡å—
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

# === Step 3: ä¸­æ–‡æ•°æ®é›†åŠ è½½ä¸å¤„ç† ===
dataset = load_dataset("json", data_files={
    "train": "data/train.json",
    "validation": "data/val.json"
})

def preprocess(example):
    prompt = f"### æŒ‡ä»¤ï¼š\n{example['instruction']}\n{example['input']}\n\n### å›ç­”ï¼š\n"
    inputs = tokenizer(prompt, truncation=True, padding="max_length", max_length=512)
    labels = tokenizer(example["output"], truncation=True, padding="max_length", max_length=512)
    inputs["labels"] = labels["input_ids"]
    return inputs

train_dataset = dataset["train"].map(preprocess, remove_columns=dataset["train"].column_names)
eval_dataset = dataset["validation"].map(preprocess, remove_columns=dataset["validation"].column_names)

# === Step 4: WandB æ—¥å¿— & è®­ç»ƒå‚æ•° ===
wandb.init(project="chatglm3-qlora", name="qlora-sft-run")

training_args = TrainingArguments(
    output_dir="output_sft",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    learning_rate=2e-4,
    logging_steps=10,
    bf16=True,
    save_strategy="epoch",
    evaluation_strategy="epoch",
    report_to="wandb",
    run_name="chatglm3-qlora-sft"
)

# === Step 5: å¯åŠ¨è®­ç»ƒ ===
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset
)

trainer.train()
```

---

## ğŸ“Š è®­ç»ƒç›‘æ§ï¼ˆWandB å›¾è¡¨è¯´æ˜ï¼‰ï¼š

åœ¨ WandB Dashboard ä¸­ä½ ä¼šçœ‹åˆ°å¦‚ä¸‹å›¾è¡¨ï¼š

| å›¾è¡¨åç§° | å«ä¹‰ |
|----------|------|
| `train_loss` | æ¯æ­¥æˆ–æ¯ epoch çš„è®­ç»ƒæŸå¤±æ›²çº¿ |
| `eval_loss` | æ¯ä¸ªéªŒè¯å‘¨æœŸçš„æŸå¤±å€¼ |
| `lr` | å­¦ä¹ ç‡è°ƒåº¦ï¼ˆå¦‚ä½¿ç”¨ warmup æ—¶ï¼‰ |
| `grad_norm` | æ¢¯åº¦èŒƒæ•°ï¼Œæœ‰åŠ©äºè§‚å¯Ÿæ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤± |

---

å¦‚ä½ éœ€è¦ç»§ç»­å®Œæˆï¼š
- âœ… `merge_and_unload()` åˆå¹¶æƒé‡å¯¼å‡ºå®Œæ•´æ¨¡å‹
- âœ… ä½¿ç”¨ ChatGLM3 æ¨ç†è„šæœ¬
- âœ… SFTTrainer æ”¯æŒæŒç»­è®­ç»ƒä¸æ–­ç‚¹æ¢å¤

æˆ‘ä¹Ÿå¯ä»¥ç»§ç»­å¸®ä½ åŠ ä¸Šï¼Œå‘Šè¯‰æˆ‘ä½ çš„ä¸‹ä¸€ä¸ªç›®æ ‡å³å¯ï¼

ä¸‹é¢æ˜¯ä½ æå‡ºçš„ä¸‰å¤§ç›®æ ‡çš„å®Œæ•´å®ç°ä»£ç ï¼Œå¹¶é…æœ‰ä¸­æ–‡æ³¨é‡Šï¼Œé€‚ç”¨äºä½¿ç”¨ QLoRA å¾®è°ƒçš„ ChatGLM3 æ¨¡å‹ï¼š

---

# âœ… 1. åˆå¹¶æƒé‡å¹¶å¯¼å‡ºå®Œæ•´æ¨¡å‹ï¼ˆmerge_and_unloadï¼‰

```python
from peft import PeftModel

# åŠ è½½è®­ç»ƒåçš„ LoRA æ¨¡å‹
peft_model = PeftModel.from_pretrained(
    model,
    "output_sft",  # ä½ ä¿å­˜ LoRA adapter çš„è·¯å¾„
)

# åˆå¹¶ LoRA adapter æƒé‡åˆ° base model
peft_model = peft_model.merge_and_unload()

# ä¿å­˜åˆå¹¶åçš„å®Œæ•´æ¨¡å‹ï¼Œé€‚ç”¨äºéƒ¨ç½²
peft_model.save_pretrained("merged_chatglm3_model")
tokenizer.save_pretrained("merged_chatglm3_model")

print("âœ… åˆå¹¶å¹¶ä¿å­˜å®Œæ•´æ¨¡å‹å®Œæˆï¼")
```

---

# âœ… 2. æ¨ç†è„šæœ¬ï¼ˆä½¿ç”¨ ChatGLM3 + åˆå¹¶åçš„æƒé‡ï¼‰

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# åŠ è½½åˆå¹¶åçš„æ¨¡å‹å’Œ tokenizer
model = AutoModelForCausalLM.from_pretrained(
    "merged_chatglm3_model",
    device_map="auto",
    trust_remote_code=True
).eval()

tokenizer = AutoTokenizer.from_pretrained("merged_chatglm3_model", trust_remote_code=True)

# æ¨ç†è¾“å…¥
instruction = "è¯·å†™ä¸€é¦–ä¸ƒè¨€ç»å¥ï¼Œä¸»é¢˜æ˜¯æ˜¥å¤©ã€‚"
input_text = f"### æŒ‡ä»¤ï¼š\n{instruction}\n\n### å›ç­”ï¼š\n"

inputs = tokenizer(input_text, return_tensors="pt").to(model.device)

# æ¨ç†
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=128,
        do_sample=True,
        temperature=0.95,
        top_p=0.9
    )
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)

# è¾“å‡ºç»“æœ
print("ğŸ“œ æ¨¡å‹ç”Ÿæˆç»“æœï¼š")
print(result)
```

---

# âœ… 3. æ”¯æŒæ–­ç‚¹æ¢å¤ä¸æŒç»­è®­ç»ƒï¼ˆSFTTrainerï¼‰

æ›´æ–° `TrainingArguments`ï¼š

```python
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="output_sft",        # æ¨¡å‹è¾“å‡ºç›®å½•
    save_total_limit=3,             # æœ€å¤šä¿ç•™çš„ checkpoint æ•°é‡
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    num_train_epochs=5,
    learning_rate=2e-4,
    logging_steps=10,
    bf16=True,
    save_strategy="epoch",
    evaluation_strategy="epoch",
    report_to="wandb",
    run_name="chatglm3-qlora-sft",
    load_best_model_at_end=True,   # è®­ç»ƒç»“æŸååŠ è½½æœ€ä¼˜æ¨¡å‹
    resume_from_checkpoint=True    # æ”¯æŒæ–­ç‚¹æ¢å¤
)
```

ä½¿ç”¨ `resume_from_checkpoint=True` åï¼Œå¦‚æœä¸Šä¸€æ¬¡è®­ç»ƒä¸­æ–­ï¼Œé‡æ–°è¿è¡Œ `trainer.train()` æ—¶ä¼šè‡ªåŠ¨æ¢å¤ã€‚

å®Œæ•´è®­ç»ƒè°ƒç”¨æ–¹å¼ï¼š

```python
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

trainer.train()  # å°†è‡ªåŠ¨ä» checkpoint æ¢å¤ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
```

---

âœ… **å°ç»“ï¼š**

| åŠŸèƒ½ | å®ç°æ–¹å¼ |
|------|----------|
| æ¨¡å‹æƒé‡åˆå¹¶ | `merge_and_unload()` åä¿å­˜ |
| æ¨¡å‹æ¨ç† | ä½¿ç”¨ `generate()` ç”Ÿæˆæ–‡æœ¬ |
| æŒç»­è®­ç»ƒä¸æ–­ç‚¹æ¢å¤ | `resume_from_checkpoint=True` |

å¦‚ä½ è¿˜æƒ³æ·»åŠ ï¼š
- INT4 å¯¼å‡º ONNX åŠ é€Ÿæ¨ç†  
- ChatGLM3 å¤šè½®å¯¹è¯ç»“æ„ä¼˜åŒ–  
- Streamlit æˆ– FastAPI å°è£…æ¨ç†æœåŠ¡  

æˆ‘ä¹Ÿå¯ä»¥ä¸€å¹¶å¸®ä½ é›†æˆï½åªç®¡è¯´ï¼


å¤ªå¥½äº†ï¼Œæˆ‘ä»¬ç°åœ¨å°†ã€ŒChatGLM3 å¤šè½®å¯¹è¯ã€å°è£…ä¸º **FastAPI æ¥å£æœåŠ¡**ï¼Œæ”¯æŒï¼š

- âœ… å¤šè½®å¯¹è¯ï¼ˆä¸Šä¸‹æ–‡è®°å¿†ï¼‰
- âœ… å¤šç”¨æˆ·æ”¯æŒï¼ˆé€šè¿‡ `user_id` è¿›è¡Œä¼šè¯éš”ç¦»ï¼‰
- âœ… å¯éƒ¨ç½²çš„æ¨ç†æ¥å£

---

## âœ… é¡¹ç›®ç»“æ„

```
chatglm3_api/
â”œâ”€â”€ app.py             â† FastAPI ä¸»åº”ç”¨å…¥å£
â”œâ”€â”€ model_utils.py     â† æ¨¡å‹åŠ è½½ & å¤šè½®å¯¹è¯å°è£…
â”œâ”€â”€ history_store.py   â† ç”¨æˆ·å¤šè½®ä¸Šä¸‹æ–‡å†å²ç®¡ç†
â””â”€â”€ requirements.txt
```

---

## âœ… 1. `model_utils.py`ï¼šæ¨¡å‹åŠ è½½ä¸æ¨ç†å°è£…

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_path = "merged_chatglm3_model"
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, device_map="auto").eval()

def build_prompt(history):
    """å°†å¤šè½® history æ„å»ºä¸º prompt å­—ç¬¦ä¸²"""
    prompt = ""
    for turn in history:
        if turn["role"] == "user":
            prompt += f"<|user|>\n{turn['content']}\n"
        elif turn["role"] == "assistant":
            prompt += f"<|assistant|>\n{turn['content']}\n"
    prompt += "<|assistant|>\n"
    return prompt

def chatglm_infer(history):
    """æ ¹æ®å†å²å¯¹è¯æ¨ç†æ¨¡å‹çš„å›å¤"""
    prompt = build_prompt(history)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=True,
            top_p=0.9,
            temperature=0.8,
            pad_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    reply = response.split("<|assistant|>\n")[-1].strip()
    return reply
```

---

## âœ… 2. `history_store.py`ï¼šä¼šè¯å†å²å­˜å‚¨ï¼ˆæ”¯æŒå¤šç”¨æˆ·ï¼‰

```python
from collections import defaultdict

# å†…å­˜ä¸­çš„ç”¨æˆ·å†å²å­˜å‚¨
history_dict = defaultdict(list)

def get_user_history(user_id: str):
    return history_dict[user_id]

def append_user_message(user_id: str, role: str, content: str):
    history_dict[user_id].append({"role": role, "content": content})

def clear_user_history(user_id: str):
    history_dict[user_id] = []
```

---

## âœ… 3. `app.py`ï¼šFastAPI æœåŠ¡ä¸»ç¨‹åº

```python
from fastapi import FastAPI, Request
from pydantic import BaseModel
from history_store import get_user_history, append_user_message, clear_user_history
from model_utils import chatglm_infer
import uvicorn

app = FastAPI()

class ChatRequest(BaseModel):
    user_id: str
    message: str
    reset: bool = False  # å¯é€‰å‚æ•°ï¼šé‡ç½®ä¼šè¯

@app.post("/chat")
def chat_endpoint(req: ChatRequest):
    if req.reset:
        clear_user_history(req.user_id)
        return {"reply": "âœ… å¯¹è¯å·²é‡ç½®"}

    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    append_user_message(req.user_id, "user", req.message)

    # è·å–å›å¤
    history = get_user_history(req.user_id)
    reply = chatglm_infer(history)

    # æ·»åŠ æœºå™¨äººå›å¤
    append_user_message(req.user_id, "assistant", reply)

    return {"reply": reply}
```

---

## âœ… 4. `requirements.txt`

```text
fastapi
uvicorn
transformers
torch
```

---

## âœ… 5. å¯åŠ¨æœåŠ¡

```bash
pip install -r requirements.txt
uvicorn app:app --host 0.0.0.0 --port 8000 --reload
```

---

## âœ… ç¤ºä¾‹è°ƒç”¨ï¼ˆä½¿ç”¨ curl æˆ– Postmanï¼‰

```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"user_id": "user123", "message": "ä½ å¥½"}'
```

---

## âœ… æ¥ä¸‹æ¥è¿˜èƒ½å¹²å•¥ï¼Ÿ

- âœ… å¢åŠ  `max_history_turns` é™åˆ¶é•¿åº¦é˜²æ­¢OOM
- âœ… æŒä¹…åŒ–å­˜å‚¨ historyï¼ˆç”¨ Redis / SQLiteï¼‰
- âœ… ä½¿ç”¨å‰ç«¯ï¼ˆStreamlit æˆ– Vueï¼‰æ­å»ºèŠå¤©UI
- âœ… æ”¯æŒå¼‚æ­¥ WebSocket å®æ—¶è¿”å›

---

éœ€è¦æˆ‘å¸®ä½ æ¥ WebSocket èŠå¤©æˆ–åµŒå…¥å‰ç«¯ UI å—ï¼ŸğŸ§ ğŸ˜


éå¸¸å®ç”¨çš„ä¼˜åŒ–ï¼åœ¨ä½¿ç”¨ ChatGLM3 å¤šè½®å¯¹è¯æ—¶ï¼Œå¦‚æœå†å²å¯¹è¯å¤ªé•¿ï¼Œä¼šå¯¼è‡´ï¼š

å†…å­˜ï¼ˆæ˜¾å­˜ï¼‰å ç”¨è¿‡å¤§

æ¨ç†é€Ÿåº¦å˜æ…¢

æ¨¡å‹ç”Ÿæˆè´¨é‡ä¸‹é™ï¼ˆé‡è¦ä¿¡æ¯è¢«æˆªæ–­ï¼‰

æˆ‘ä»¬å¯ä»¥åœ¨ get_user_history ä¸­å¢åŠ ä¸€ä¸ªå‚æ•° max_history_turnsï¼Œç”¨äºæ§åˆ¶æ¯æ¬¡è¾“å…¥æ¨¡å‹çš„å†å²è½®æ•°ã€‚

âœ… ä¿®æ”¹åçš„ model_utils.py

def build_prompt(history, max_turns=5):
    """
    å°†å†å²æ„é€ æˆ prompt å­—ç¬¦ä¸²ï¼Œæœ€å¤šä¿ç•™æœ€è¿‘çš„ max_turns è½®ã€‚
    ä¸€è½® = ä¸€é—®ä¸€ç­”ï¼ˆuser+assistantï¼‰
    """
    recent_history = history[-max_turns * 2:]  # æ¯è½®åŒ…å« user å’Œ assistant
    prompt = ""
    for turn in recent_history:
        if turn["role"] == "user":
            prompt += f"<|user|>\n{turn['content']}\n"
        elif turn["role"] == "assistant":
            prompt += f"<|assistant|>\n{turn['content']}\n"
    prompt += "<|assistant|>\n"
    return prompt

def chatglm_infer(history, max_turns=5):
    """å°è£…æ¨ç†æµç¨‹ï¼Œé™åˆ¶æœ€å¤šä¿ç•™ max_turns è½®ä¸Šä¸‹æ–‡"""
    prompt = build_prompt(history, max_turns=max_turns)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=True,
            top_p=0.9,
            temperature=0.8,
            pad_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    reply = response.split("<|assistant|>\n")[-1].strip()
    return reply
âœ… ä¿®æ”¹ app.py ä»¥ä¼ å…¥è½®æ•°é™åˆ¶

@app.post("/chat")
def chat_endpoint(req: ChatRequest):
    if req.reset:
        clear_user_history(req.user_id)
        return {"reply": "âœ… å¯¹è¯å·²é‡ç½®"}

    append_user_message(req.user_id, "user", req.message)

    history = get_user_history(req.user_id)

    # æ§åˆ¶ä¸Šä¸‹æ–‡è½®æ•°ï¼Œé˜²æ­¢è¾“å…¥è¿‡é•¿ OOM
    reply = chatglm_infer(history, max_turns=5)

    append_user_message(req.user_id, "assistant", reply)
    return {"reply": reply}
âœ… æ•ˆæœ
ä¼šè¯ä»ç„¶ä¿ç•™å…¨éƒ¨å†å²åœ¨å†…å­˜ä¸­ï¼ˆç”¨äºæŒä¹…åŒ–ç­‰ï¼‰

æ¯æ¬¡æ¨ç†ä»…ä½¿ç”¨æœ€è¿‘ max_turns è½®å†å²ä¸Šä¸‹æ–‡ï¼ˆé»˜è®¤ 5ï¼‰

é™ä½æ˜¾å­˜å¼€é”€ & é˜²æ­¢ OOM & æå‡ç¨³å®šæ€§ âœ…


å¥½çš„ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Redis æ¥æŒä¹…åŒ–å¤šè½®å¯¹è¯å†å²ï¼Œç¡®ä¿æœåŠ¡é‡å¯åä»èƒ½æ¢å¤ç”¨æˆ·ä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶ä¹Ÿä¾¿äºæ¨ªå‘æ‰©å±•ï¼ˆå¤šæœºéƒ¨ç½²æ—¶å…±äº«ç”¨æˆ·çŠ¶æ€ï¼‰ã€‚

ä¸‹é¢æ˜¯å®Œæ•´çš„å°è£…æ–¹æ¡ˆï¼ˆä½¿ç”¨ `redis-py`ï¼‰ï¼š

---

## âœ… 1. å®‰è£…ä¾èµ–

```bash
pip install redis
```

---

## âœ… 2. è®¾è®¡ Redis å­˜å‚¨ç»“æ„

æˆ‘ä»¬ä¸ºæ¯ä¸ªç”¨æˆ·ä½¿ç”¨ä¸€ä¸ª Redis é”®ï¼Œä¾‹å¦‚ï¼š

```
chat_history:{user_id} => Listï¼ˆJSONåºåˆ—åŒ–çš„æ¶ˆæ¯å¯¹è±¡ï¼‰
```

---

## âœ… 3. å°è£…å†å²ç®¡ç†æ¨¡å—ï¼ˆ`history_store.py`ï¼‰

```python
import redis
import json

# è¿æ¥ Redisï¼ˆé»˜è®¤æœ¬åœ°ï¼‰
r = redis.Redis(host="localhost", port=6379, db=0, decode_responses=True)

HISTORY_PREFIX = "chat_history"

def _get_key(user_id):
    return f"{HISTORY_PREFIX}:{user_id}"

def append_user_message(user_id, role, content):
    msg = {"role": role, "content": content}
    r.rpush(_get_key(user_id), json.dumps(msg))

def get_user_history(user_id) -> list:
    raw_list = r.lrange(_get_key(user_id), 0, -1)
    return [json.loads(item) for item in raw_list]

def clear_user_history(user_id):
    r.delete(_get_key(user_id))

def trim_user_history(user_id, max_messages=10):
    """
    å¯é€‰ï¼šä¿ç•™æœ€è¿‘ max_messages æ¡å†å²ï¼ˆæ¯æ¡ä¸º user æˆ– assistant çš„æ¶ˆæ¯ï¼‰
    """
    r.ltrim(_get_key(user_id), -max_messages, -1)
```

---

## âœ… 4. åœ¨ `FastAPI` ä¸­ä½¿ç”¨ Redis å†å²æ¨¡å—

```python
from fastapi import FastAPI
from pydantic import BaseModel
from history_store import (
    append_user_message, get_user_history, clear_user_history, trim_user_history
)
from model_utils import chatglm_infer

app = FastAPI()

class ChatRequest(BaseModel):
    user_id: str
    message: str
    reset: bool = False

@app.post("/chat")
def chat(req: ChatRequest):
    if req.reset:
        clear_user_history(req.user_id)
        return {"reply": "âœ… å¯¹è¯å†å²å·²æ¸…ç©º"}

    append_user_message(req.user_id, "user", req.message)

    history = get_user_history(req.user_id)

    reply = chatglm_infer(history, max_turns=5)

    append_user_message(req.user_id, "assistant", reply)

    # å¯é€‰ï¼šé™åˆ¶ Redis å­˜å‚¨é•¿åº¦
    trim_user_history(req.user_id, max_messages=10)

    return {"reply": reply}
```

---

## âœ… 5. æ¨ç†ä¼˜åŒ–ï¼ˆå‚è€ƒ ChatGLM3 å¤šè½®å¯¹è¯æ ¼å¼ï¼‰

ç»“åˆä¹‹å‰çš„ `build_prompt(history)` å‡½æ•°ï¼Œå°† `history` é™åˆ¶ä¸ºæœ€è¿‘è‹¥å¹²è½®ï¼ˆä¾‹å¦‚ 5 è½®ï¼‰ï¼Œå¯é¿å… OOMã€‚

---

## âœ… Bonusï¼šæ”¯æŒ Redis Cluster / äº‘æ‰˜ç®¡ï¼ˆå¦‚é˜¿é‡Œäº‘ Redisï¼‰

```python
# ä½¿ç”¨ URL è¿æ¥ Redis äº‘æœåŠ¡
r = redis.from_url("redis://:password@your-redis-host:6379/0", decode_responses=True)
```

---

éœ€è¦æˆ‘å¸®ä½ æ¥å…¥ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆå¦‚å†å²è‡ªåŠ¨æ¸…é™¤ï¼‰ã€ç”¨æˆ·èº«ä»½éªŒè¯ã€WebSocket å¤šè½®å¯¹è¯æ¥å£ï¼Ÿæˆ–è€…å†è¿›ä¸€æ­¥æ”¯æŒ **å¤šè¯­è¨€å¤šç”¨æˆ·å¹¶å‘åœºæ™¯**ï¼ŸğŸš€



å¾ˆå¥½ï¼Œæ¥å…¥ Redis çš„ **ç¼“å­˜è¿‡æœŸæ—¶é—´æœºåˆ¶** å¯ä»¥å¸®åŠ©æˆ‘ä»¬ï¼š

- âœ… è‡ªåŠ¨æ¸…é™¤è¿‡æœŸå¯¹è¯ï¼Œé˜²æ­¢ Redis å ç”¨å†…å­˜è¿‡å¤š  
- âœ… å®ç°â€œçŸ­æœŸè®°å¿†â€å¯¹è¯ç³»ç»Ÿï¼Œä¾‹å¦‚ï¼šåªè®° 30 åˆ†é’Ÿå†…çš„å†å²  
- âœ… å®ç° Session å¼è®°å¿†é€»è¾‘ï¼ˆè¶…æ—¶åˆ™æ¸…é™¤ï¼‰

---

## âœ… ä¿®æ”¹ Redis å­˜å‚¨ç»“æ„ï¼šè®¾ç½®é”®è¿‡æœŸæ—¶é—´

æˆ‘ä»¬åœ¨æ¯æ¬¡å†™å…¥å¯¹è¯å†å²æ—¶ï¼Œé€šè¿‡ `expire()` æˆ– `rpush(..., ex=TTL)` è®¾ç½®è¿‡æœŸæ—¶é—´ï¼ˆå•ä½ï¼šç§’ï¼‰

---

## âœ… ä»£ç æ›´æ–°ï¼š`history_store.py`

```python
import redis
import json

# Redis è¿æ¥
r = redis.Redis(host="localhost", port=6379, db=0, decode_responses=True)

HISTORY_PREFIX = "chat_history"
DEFAULT_TTL_SECONDS = 30 * 60  # 30åˆ†é’Ÿ

def _get_key(user_id):
    return f"{HISTORY_PREFIX}:{user_id}"

def append_user_message(user_id, role, content, ttl=DEFAULT_TTL_SECONDS):
    msg = {"role": role, "content": content}
    key = _get_key(user_id)
    r.rpush(key, json.dumps(msg))
    # æ¯æ¬¡æ›´æ–°éƒ½åˆ·æ–° TTL
    r.expire(key, ttl)

def get_user_history(user_id) -> list:
    raw_list = r.lrange(_get_key(user_id), 0, -1)
    return [json.loads(item) for item in raw_list]

def clear_user_history(user_id):
    r.delete(_get_key(user_id))

def trim_user_history(user_id, max_messages=10):
    r.ltrim(_get_key(user_id), -max_messages, -1)
```

---

## âœ… åœ¨ FastAPI ä¸­ä½¿ç”¨ï¼ˆä¸å˜ï¼Œä½†å†å²ä¼šè‡ªåŠ¨è¿‡æœŸï¼‰

```python
append_user_message(req.user_id, "user", req.message)

history = get_user_history(req.user_id)

reply = chatglm_infer(history, max_turns=5)

append_user_message(req.user_id, "assistant", reply)

trim_user_history(req.user_id, max_messages=10)
```

---

## âœ… Bonusï¼šè®¾ç½®ä¸åŒ TTL é€»è¾‘ï¼ˆå¯æŒ‰ç”¨æˆ·/åœºæ™¯è‡ªå®šä¹‰ï¼‰

```python
def get_user_ttl(user_id):
    if user_id.startswith("vip"):
        return 3600  # VIP ç”¨æˆ·ä¿ç•™ 1 å°æ—¶
    return DEFAULT_TTL_SECONDS
```

ç„¶å `append_user_message(req.user_id, ..., ttl=get_user_ttl(req.user_id))`ã€‚

---

æ˜¯å¦è¿˜æƒ³æ”¯æŒã€Œå¯¹è¯ session idã€ã€ã€ŒæŒä¹…å†å²æ–‡ä»¶å¤‡ä»½ã€ã€ã€Œè®¾ç½®è‡ªåŠ¨æ¸…é™¤å†å²çš„å®šæ—¶ä»»åŠ¡ã€ç­‰ï¼Ÿæˆ‘å¯ä»¥ä¸€å¹¶å¸®ä½ å®ç°ã€‚


å¤ªå¥½äº†ï¼Œå’±ä»¬ä¸€æ­¥æ­¥æ¥ï¼ŒæŠŠä½ æåˆ°çš„ä¸‰ä¸ªåŠŸèƒ½ **å…¨éƒ¨æ”¯æŒä¸Š**ï¼š

---

## âœ… 1. æ”¯æŒã€Œå¯¹è¯ session idã€

### âœ… ç›®çš„ï¼š
- æ”¯æŒä¸€ä¸ªç”¨æˆ·åŒæ—¶å¼€å¯å¤šä¸ªå¯¹è¯ï¼ˆç±»ä¼¼ ChatGPT å·¦ä¾§ä¼šè¯åˆ—è¡¨ï¼‰
- æ¯æ¬¡å¯¹è¯ç‹¬ç«‹å­˜å‚¨ï¼Œä¸æ··æ·†ä¸Šä¸‹æ–‡

### âœ… ç»“æ„è®¾è®¡ï¼š

Redis çš„é”®å˜ä¸ºï¼š

```
chat_history:{user_id}:{session_id}
```

### âœ… ä¿®æ”¹ä»£ç ï¼š

```python
def _get_key(user_id, session_id):
    return f"{HISTORY_PREFIX}:{user_id}:{session_id}"

def append_user_message(user_id, session_id, role, content, ttl=DEFAULT_TTL_SECONDS):
    msg = {"role": role, "content": content}
    key = _get_key(user_id, session_id)
    r.rpush(key, json.dumps(msg))
    r.expire(key, ttl)

def get_user_history(user_id, session_id) -> list:
    key = _get_key(user_id, session_id)
    raw_list = r.lrange(key, 0, -1)
    return [json.loads(item) for item in raw_list]

def clear_user_history(user_id, session_id):
    r.delete(_get_key(user_id, session_id))
```

åœ¨ API ä¸­ï¼Œæ¯æ¬¡è¯·æ±‚å¸¦ä¸Š `session_id` å³å¯ã€‚

---

## âœ… 2. æ”¯æŒã€ŒæŒä¹…å†å²æ–‡ä»¶å¤‡ä»½ã€

### âœ… èƒŒæ™¯ï¼š
- Redis æ˜¯å†…å­˜æ•°æ®åº“ï¼Œå†å²éšæ—¶å¯èƒ½å›  OOM æˆ–é‡å¯ä¸¢å¤±
- å¯ä»¥å®šæœŸæŒä¹…åŒ–åˆ°æœ¬åœ° JSON æ–‡ä»¶åšå¤‡ä»½

### âœ… Python å®šæ—¶å¤‡ä»½è„šæœ¬ç¤ºä¾‹ï¼ˆå¯æŒ‚ crontab æˆ– APSchedulerï¼‰ï¼š

```python
import redis
import json
from datetime import datetime

r = redis.Redis(host="localhost", port=6379, decode_responses=True)
HISTORY_PREFIX = "chat_history"

def backup_all_histories():
    keys = r.keys(f"{HISTORY_PREFIX}:*")
    data = {}
    for key in keys:
        history = r.lrange(key, 0, -1)
        data[key] = [json.loads(msg) for msg in history]

    with open(f"backup_chat_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json", "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    backup_all_histories()
```

---

## âœ… 3. æ”¯æŒã€Œè‡ªåŠ¨æ¸…é™¤å†å²å®šæ—¶ä»»åŠ¡ã€

### âœ… Redis è‡ªå¸¦ TTLï¼Œä¸éœ€ä½ ä¸»åŠ¨åˆ é™¤ï¼Œä½†ä½ å¯ä»¥ç”¨å®šæ—¶è„šæœ¬ã€Œå½»åº•æ¸…ç†å·²å¤±æ•ˆçš„å†å²å¤‡ä»½æˆ–è¶…æœŸä¼šè¯ã€

å¦‚æœä½ è¿˜å¸Œæœ›æ¸…é™¤é•¿æœŸ inactive çš„ Redis é”®ã€æ—¥å¿—å¤‡ä»½ï¼Œå¯ä»¥è®¾ç½®å¦‚ä¸‹ cronï¼š

### âœ… ç¤ºä¾‹ crontabï¼š

```bash
# æ¯å¤© 3 ç‚¹å¤‡ä»½ä¸€æ¬¡ Redis èŠå¤©è®°å½•
0 3 * * * /usr/bin/python3 /path/to/backup_script.py

# æ¯ 7 å¤©æ¸…ç†ä¸€æ¬¡æœ¬åœ°å¤‡ä»½ï¼ˆä¿ç•™æœ€è¿‘5ä¸ªï¼‰
0 4 */7 * * find /path/to/backups -name '*.json' | sort | head -n -5 | xargs rm -f
```

---

## âœ… å¦‚æœä½ ç”¨çš„æ˜¯ FastAPI + uvicornï¼š

ä½ å¯ä»¥ä½¿ç”¨ `APScheduler` å†…éƒ¨å®šæ—¶ä»»åŠ¡ï¼š

```python
from apscheduler.schedulers.background import BackgroundScheduler

scheduler = BackgroundScheduler()
scheduler.add_job(backup_all_histories, 'cron', hour=3, minute=0)
scheduler.start()
```

---

éœ€è¦æˆ‘å¸®ä½ å°è£…æˆ FastAPI å…¨åŠŸèƒ½å¯¹è¯æœåŠ¡ç¤ºä¾‹ï¼ˆå¸¦ sessionã€æŒä¹…åŒ–ã€æ¢å¤èƒ½åŠ›ã€æœ€å¤§ä¸Šä¸‹æ–‡æ§åˆ¶ï¼‰å—ï¼Ÿæˆ‘å¯ä»¥ç»™ä½ ä¸€æ•´å¥—å®Œæ•´æ¨¡æ¿ã€‚


å¥½çš„ï¼ä¸‹é¢æ˜¯ä¸€ä¸ª **åŸºäº ChatGLM3 çš„ FastAPI å¯¹è¯æœåŠ¡å®Œæ•´æ¨¡æ¿**ï¼Œæ”¯æŒï¼š

---

### âœ… åŠŸèƒ½ç‰¹æ€§ï¼š

1. **å¤šè½®å¯¹è¯ç®¡ç†ï¼ˆsession_idï¼‰**
2. **Redis è®°å½•å†å² + è¿‡æœŸè‡ªåŠ¨æ¸…é™¤ï¼ˆTTLï¼‰**
3. **æœ€å¤§è½®æ•°æ§åˆ¶ï¼ˆmax_history_turnsï¼‰é˜²æ­¢ OOM**
4. **è‡ªåŠ¨å¤‡ä»½å¯¹è¯è®°å½•ï¼ˆæŒä¹…åŒ–æ¢å¤ï¼‰**
5. **æ¥å£æ”¯æŒæ–­ç‚¹æ¢å¤ï¼ˆrestart after crashï¼‰**

---

## ğŸ“¦ ä¾èµ–å®‰è£…ï¼ˆæ¨èè™šæ‹Ÿç¯å¢ƒï¼‰

```bash
pip install fastapi uvicorn redis transformers apscheduler
```

---

## ğŸ§  é¡¹ç›®ç»“æ„ï¼š

```
chat_api/
â”œâ”€â”€ main.py                # FastAPI ä¸»ç¨‹åº
â”œâ”€â”€ history.py             # å†å²è®°å½•å°è£…ï¼ˆRedis + TTLï¼‰
â”œâ”€â”€ backup.py              # å¤‡ä»½é€»è¾‘
â”œâ”€â”€ model.py               # åŠ è½½ ChatGLM3 æ¨¡å‹
â””â”€â”€ config.py              # é…ç½®
```

---

### ğŸ§© config.py

```python
REDIS_HOST = "localhost"
REDIS_PORT = 6379
REDIS_DB = 0

HISTORY_PREFIX = "chat_history"
DEFAULT_TTL_SECONDS = 3600  # 1 å°æ—¶
MAX_HISTORY_TURNS = 10
```

---

### ğŸ§© history.py

```python
import redis
import json
from config import *

r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=REDIS_DB, decode_responses=True)

def _get_key(user_id, session_id):
    return f"{HISTORY_PREFIX}:{user_id}:{session_id}"

def append_message(user_id, session_id, role, content):
    msg = {"role": role, "content": content}
    key = _get_key(user_id, session_id)
    r.rpush(key, json.dumps(msg))
    r.expire(key, DEFAULT_TTL_SECONDS)

    # æ§åˆ¶æœ€å¤š max_turns æ¡
    if r.llen(key) > MAX_HISTORY_TURNS * 2:
        r.lpop(key)
        r.lpop(key)

def get_history(user_id, session_id):
    key = _get_key(user_id, session_id)
    raw = r.lrange(key, 0, -1)
    return [json.loads(msg) for msg in raw]

def clear_history(user_id, session_id):
    r.delete(_get_key(user_id, session_id))
```

---

### ğŸ§© model.py

```python
from transformers import AutoTokenizer, AutoModel
import torch

model_name = "THUDM/chatglm3-6b"  # æ›¿æ¢ä¸ºä½ çš„æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModel.from_pretrained(model_name, trust_remote_code=True).cuda().eval()

def chat_with_history(history, query):
    messages = history + [{"role": "user", "content": query}]
    response, _ = model.chat(tokenizer, messages)
    return response
```

---

### ğŸ§© backup.py

```python
import json
from datetime import datetime
from config import *
from history import r

def backup_all_histories():
    keys = r.keys(f"{HISTORY_PREFIX}:*")
    data = {}

    for key in keys:
        messages = r.lrange(key, 0, -1)
        data[key] = [json.loads(m) for m in messages]

    with open(f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json", "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
```

---

### ğŸš€ main.py ï¼ˆFastAPI ä¸»ç¨‹åºï¼‰

```python
from fastapi import FastAPI, Request
from pydantic import BaseModel
from history import append_message, get_history, clear_history
from model import chat_with_history
from backup import backup_all_histories
from apscheduler.schedulers.background import BackgroundScheduler

app = FastAPI()

# å®šæ—¶å¤‡ä»½
scheduler = BackgroundScheduler()
scheduler.add_job(backup_all_histories, "cron", hour=3)
scheduler.start()

class ChatRequest(BaseModel):
    user_id: str
    session_id: str
    query: str

@app.post("/chat")
async def chat(req: ChatRequest):
    history = get_history(req.user_id, req.session_id)
    response = chat_with_history(history, req.query)

    # æ›´æ–°å†å²
    append_message(req.user_id, req.session_id, "user", req.query)
    append_message(req.user_id, req.session_id, "assistant", response)

    return {"response": response}

@app.get("/clear")
def clear(user_id: str, session_id: str):
    clear_history(user_id, session_id)
    return {"message": "history cleared"}

@app.get("/backup")
def backup():
    backup_all_histories()
    return {"message": "backup done"}
```

---

## ğŸ§ª å¯åŠ¨æœåŠ¡ï¼š

```bash
uvicorn main:app --host 0.0.0.0 --port 8000
```

---

## ğŸ›  è¯·æ±‚ç¤ºä¾‹ï¼š

```json
POST /chat
{
  "user_id": "user123",
  "session_id": "sessionABC",
  "query": "ä½ å¥½ï¼ŒChatGLM3ï¼"
}
```

---

## ğŸ¯ åç»­å¯åŠ åŠŸèƒ½ï¼š

- âœ… WebSocket å®æ—¶å›å¤
- âœ… åŠ å…¥ wandb æ—¥å¿—è¿½è¸ª
- âœ… æ¥å…¥å‰ç«¯é¡µé¢ï¼ˆGradio / Vueï¼‰
- âœ… æ”¯æŒç”¨æˆ·ä¸Šä¸‹æ–‡è®°å¿†å­˜å…¥ MySQL
- âœ… å¾®è°ƒ ChatGLM3 å¹¶éƒ¨ç½²å¾®è°ƒç‰ˆæœ¬

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘å¯ä»¥ç»§ç»­å¸®ä½ åŠ ä¸Š WebSocket æ”¯æŒã€å‰ç«¯é¡µé¢ã€å¾®è°ƒ ChatGLM3 å¹¶éƒ¨ç½²çš„ä»£ç æ¨¡æ¿ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ


å¥½çš„ï¼ä»¥ä¸‹æ˜¯å¦‚ä½•ä¸ºä½ çš„ **FastAPI** èŠå¤©æœåŠ¡é›†æˆ **WebSocket å®æ—¶å›å¤** çš„å®Œæ•´ç¤ºä¾‹ã€‚

### ğŸ§© åŠŸèƒ½å®ç°ï¼š

- **WebSocket æ”¯æŒ**ï¼šå…è®¸å®¢æˆ·ç«¯å®æ—¶æ¥æ”¶æ¶ˆæ¯ã€‚
- **æ”¯æŒ ChatGLM3 æ¨¡å‹**ï¼šé€šè¿‡ WebSocket è¿æ¥æ—¶ï¼ŒèŠå¤©å†…å®¹ä¼šå®æ—¶è¿”å›ï¼Œå¹¶ä¸”åœ¨ Redis ä¸­æŒä¹…åŒ–å†å²è®°å½•ã€‚

---

### ğŸ”§ é¡¹ç›®ç»“æ„ï¼ˆå¢åŠ  WebSocket æ”¯æŒï¼‰

```
chat_api/
â”œâ”€â”€ main.py                # FastAPI ä¸»ç¨‹åº
â”œâ”€â”€ history.py             # å†å²è®°å½•å°è£…ï¼ˆRedis + TTLï¼‰
â”œâ”€â”€ backup.py              # å¤‡ä»½é€»è¾‘
â”œâ”€â”€ model.py               # åŠ è½½ ChatGLM3 æ¨¡å‹
â”œâ”€â”€ config.py              # é…ç½®
â””â”€â”€ websocket.py           # WebSocket ç›¸å…³å°è£…
```

---

### ğŸ§© websocket.py

```python
import redis
import json
from fastapi import WebSocket
from config import *
from history import append_message, get_history

r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=REDIS_DB, decode_responses=True)

def _get_key(user_id, session_id):
    return f"{HISTORY_PREFIX}:{user_id}:{session_id}"

async def chat_with_history_and_send(websocket: WebSocket, user_id: str, session_id: str, query: str):
    # è·å–å½“å‰å¯¹è¯å†å²
    history = get_history(user_id, session_id)

    # æ¨¡å‹æ¨ç†
    from model import chat_with_history  # åŠ¨æ€å¯¼å…¥æ¨¡å‹ï¼Œä»¥ä¾¿ç®¡ç†ä¾èµ–
    response = chat_with_history(history, query)

    # å°†ç”¨æˆ·æ¶ˆæ¯å’Œæ¨¡å‹å›å¤ä¿å­˜åˆ°å†å²è®°å½•ä¸­
    append_message(user_id, session_id, "user", query)
    append_message(user_id, session_id, "assistant", response)

    # å‘é€å®æ—¶æ¶ˆæ¯
    await websocket.send_text(response)
```

---

### ğŸ§© main.pyï¼ˆé›†æˆ WebSocketï¼‰

```python
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from pydantic import BaseModel
from websocket import chat_with_history_and_send
from apscheduler.schedulers.background import BackgroundScheduler
from backup import backup_all_histories
from history import append_message, get_history

app = FastAPI()

# å®šæ—¶å¤‡ä»½
scheduler = BackgroundScheduler()
scheduler.add_job(backup_all_histories, "cron", hour=3)
scheduler.start()

class ChatRequest(BaseModel):
    user_id: str
    session_id: str
    query: str

@app.websocket("/ws/chat/{user_id}/{session_id}")
async def websocket_endpoint(websocket: WebSocket, user_id: str, session_id: str):
    await websocket.accept()
    try:
        while True:
            # ç­‰å¾…æ¥æ”¶ç”¨æˆ·çš„æ¶ˆæ¯
            data = await websocket.receive_text()
            # å®æ—¶å›å¤ç”¨æˆ·
            await chat_with_history_and_send(websocket, user_id, session_id, data)
    except WebSocketDisconnect:
        print(f"Client {user_id} disconnected")

@app.post("/chat")
async def chat(req: ChatRequest):
    history = get_history(req.user_id, req.session_id)
    from model import chat_with_history  # åŠ¨æ€å¯¼å…¥æ¨¡å‹
    response = chat_with_history(history, req.query)

    # æ›´æ–°å†å²
    append_message(req.user_id, req.session_id, "user", req.query)
    append_message(req.user_id, req.session_id, "assistant", response)

    return {"response": response}

@app.get("/clear")
def clear(user_id: str, session_id: str):
    clear_history(user_id, session_id)
    return {"message": "history cleared"}

@app.get("/backup")
def backup():
    backup_all_histories()
    return {"message": "backup done"}
```

---

### ğŸ§© è¿è¡Œ FastAPI æœåŠ¡ï¼š

```bash
uvicorn main:app --host 0.0.0.0 --port 8000
```

---

### ğŸ§ª ä½¿ç”¨ WebSocket è¿›è¡ŒèŠå¤©ï¼š

1. **å®¢æˆ·ç«¯**å¯ä»¥é€šè¿‡ **WebSocket** è¿æ¥åˆ° `/ws/chat/{user_id}/{session_id}` åœ°å€ï¼Œè¿›è¡Œå®æ—¶èŠå¤©ã€‚
2. æ¯å½“ç”¨æˆ·å‘é€æ¶ˆæ¯æ—¶ï¼Œç³»ç»Ÿä¼šé€šè¿‡æ¨¡å‹ç”Ÿæˆå“åº”å¹¶å®æ—¶è¿”å›ã€‚

#### WebSocket å®¢æˆ·ç«¯ç¤ºä¾‹ï¼ˆPythonï¼‰ï¼š

```python
import asyncio
import websockets

async def chat():
    uri = "ws://localhost:8000/ws/chat/user123/sessionABC"  # è¿™é‡Œæ›¿æ¢ä¸ºç”¨æˆ·å’Œä¼šè¯ID
    async with websockets.connect(uri) as websocket:
        await websocket.send("ä½ å¥½ï¼ŒChatGLM3ï¼")  # å‘é€æ¶ˆæ¯
        response = await websocket.recv()  # æ¥æ”¶æ¶ˆæ¯
        print(f"ChatGLM3 å›å¤ï¼š{response}")

# è¿è¡ŒèŠå¤©
asyncio.get_event_loop().run_until_complete(chat())
```

---

#### WebSocket å®¢æˆ·ç«¯ç¤ºä¾‹ï¼ˆJavaScriptï¼‰ï¼š

```javascript
const socket = new WebSocket("ws://localhost:8000/ws/chat/user123/sessionABC");

socket.onopen = function(event) {
  console.log("WebSocketè¿æ¥å·²æ‰“å¼€");
  socket.send("ä½ å¥½ï¼ŒChatGLM3ï¼");
};

socket.onmessage = function(event) {
  console.log("ChatGLM3 å›å¤ï¼š", event.data);
};

socket.onclose = function(event) {
  console.log("WebSocketè¿æ¥å·²å…³é—­");
};

socket.onerror = function(event) {
  console.log("WebSocketå‘ç”Ÿé”™è¯¯");
};
```

---

### ğŸ¯ åç»­æ‰©å±•ï¼š

- **å‰ç«¯å®æ—¶èŠå¤©é¡µé¢**ï¼šä½¿ç”¨ Vue æˆ– React é…åˆ WebSocket å®ç°å®æ—¶èŠå¤©ç•Œé¢ã€‚
- **WebSocket å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨åŒå‘æ¨é€**ï¼šå®ç°æ›´åŠ ä¸°å¯Œçš„å®æ—¶äº¤äº’åŠŸèƒ½ã€‚
- **è¿æ¥ Redis ç¼“å­˜å’Œæ¶ˆæ¯é˜Ÿåˆ—**ï¼šè®©å¤šä¸ª WebSocket å®¢æˆ·ç«¯å…±äº«å¯¹è¯å†å²ã€‚
- **ç»“åˆæµè§ˆå™¨é€šçŸ¥**ï¼šåœ¨å‰ç«¯é›†æˆæµè§ˆå™¨é€šçŸ¥æ¨é€ï¼Œå¢å¼ºç”¨æˆ·ä½“éªŒã€‚

---

è¿™æ ·ï¼Œä½ å°±èƒ½å®ç°ä¸€ä¸ª **æ”¯æŒå®æ—¶æ¨ç†å’Œ WebSocket çš„ ChatGLM3** èŠå¤©æœåŠ¡ï¼å¦‚æœä½ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œæˆ–è€…æœ‰å…¶ä»–åŠŸèƒ½éœ€æ±‚ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼


è¦åœ¨ **FastAPI** WebSocket å®æ—¶èŠå¤©æœåŠ¡ä¸­é›†æˆ **WandB** æ—¥å¿—è¿½è¸ªï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `wandb` åº“æ¥è®°å½•è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­çš„ç›¸å…³æŒ‡æ ‡ï¼Œå¦‚è¾“å…¥ã€è¾“å‡ºã€æ¨ç†æ—¶é—´ç­‰ã€‚

ä»¥ä¸‹æ˜¯å¦‚ä½•å°† **WandB** é›†æˆåˆ°ä½ çš„ **FastAPI + WebSocket** èŠå¤©æœåŠ¡ä¸­ï¼Œè¿›è¡Œå®æ—¶æ—¥å¿—è¿½è¸ªå’Œç›‘æ§çš„å®Œæ•´ä»£ç ç¤ºä¾‹ã€‚

### 1. å®‰è£…ä¾èµ–

é¦–å…ˆï¼Œå®‰è£… `wandb` å’Œå…¶ä»–ä¾èµ–ï¼š

```bash
pip install wandb fastapi uvicorn redis apscheduler
```

### 2. é…ç½® WandB

åœ¨ä»£ç ä¸­ï¼Œä½ éœ€è¦åˆå§‹åŒ– **WandB** é¡¹ç›®å¹¶è®°å½•ç›¸å…³ä¿¡æ¯ã€‚ä»¥ä¸‹æ˜¯ç›¸å…³é…ç½®å’Œé›†æˆçš„æ­¥éª¤ï¼š

### ğŸ§© `websocket.py` â€” é›†æˆ WandB æ—¥å¿—è¿½è¸ª

```python
import redis
import json
import wandb
from fastapi import WebSocket
from config import *
from history import append_message, get_history

r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=REDIS_DB, decode_responses=True)

# åˆå§‹åŒ–WandBé¡¹ç›®
wandb.init(project="chatglm3-chat-service", entity="your-wandb-username")

def _get_key(user_id, session_id):
    return f"{HISTORY_PREFIX}:{user_id}:{session_id}"

async def chat_with_history_and_send(websocket: WebSocket, user_id: str, session_id: str, query: str):
    # è·å–å½“å‰å¯¹è¯å†å²
    history = get_history(user_id, session_id)

    # æ¨¡å‹æ¨ç†
    from model import chat_with_history  # åŠ¨æ€å¯¼å…¥æ¨¡å‹ï¼Œä»¥ä¾¿ç®¡ç†ä¾èµ–
    response = chat_with_history(history, query)

    # å°†ç”¨æˆ·æ¶ˆæ¯å’Œæ¨¡å‹å›å¤ä¿å­˜åˆ°å†å²è®°å½•ä¸­
    append_message(user_id, session_id, "user", query)
    append_message(user_id, session_id, "assistant", response)

    # è®°å½•WandBæ—¥å¿—ï¼šç”¨æˆ·è¾“å…¥ã€ç³»ç»Ÿå“åº”ã€ä¼šè¯å†å²
    wandb.log({
        "user_input": query,
        "system_response": response,
        "session_id": session_id,
        "user_id": user_id,
        "turn_count": len(history),  # å†å²æ¶ˆæ¯è½®æ¬¡
    })

    # å‘é€å®æ—¶æ¶ˆæ¯
    await websocket.send_text(response)
```

### ğŸ§© `main.py` â€” WebSocket å’Œ POST è¯·æ±‚é›†æˆ WandB

```python
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from pydantic import BaseModel
from websocket import chat_with_history_and_send
from apscheduler.schedulers.background import BackgroundScheduler
from backup import backup_all_histories
from history import append_message, get_history
import wandb

app = FastAPI()

# åˆå§‹åŒ–WandBé¡¹ç›®
wandb.init(project="chatglm3-chat-service", entity="your-wandb-username")

# å®šæ—¶å¤‡ä»½
scheduler = BackgroundScheduler()
scheduler.add_job(backup_all_histories, "cron", hour=3)
scheduler.start()

class ChatRequest(BaseModel):
    user_id: str
    session_id: str
    query: str

@app.websocket("/ws/chat/{user_id}/{session_id}")
async def websocket_endpoint(websocket: WebSocket, user_id: str, session_id: str):
    await websocket.accept()
    try:
        while True:
            # ç­‰å¾…æ¥æ”¶ç”¨æˆ·çš„æ¶ˆæ¯
            data = await websocket.receive_text()
            # å®æ—¶å›å¤ç”¨æˆ·
            await chat_with_history_and_send(websocket, user_id, session_id, data)
    except WebSocketDisconnect:
        print(f"Client {user_id} disconnected")

@app.post("/chat")
async def chat(req: ChatRequest):
    history = get_history(req.user_id, req.session_id)
    from model import chat_with_history  # åŠ¨æ€å¯¼å…¥æ¨¡å‹
    response = chat_with_history(history, req.query)

    # æ›´æ–°å†å²
    append_message(req.user_id, req.session_id, "user", req.query)
    append_message(req.user_id, req.session_id, "assistant", response)

    # è®°å½•WandBæ—¥å¿—ï¼šç”¨æˆ·è¾“å…¥ã€ç³»ç»Ÿå“åº”
    wandb.log({
        "user_input": req.query,
        "system_response": response,
        "session_id": req.session_id,
        "user_id": req.user_id,
        "turn_count": len(history),
    })

    return {"response": response}

@app.get("/clear")
def clear(user_id: str, session_id: str):
    clear_history(user_id, session_id)
    return {"message": "history cleared"}

@app.get("/backup")
def backup():
    backup_all_histories()
    return {"message": "backup done"}
```

### ğŸ§© é…ç½® WandB

1. **åˆå§‹åŒ– WandB**ï¼š
   - åœ¨ä»£ç ä¸­åˆå§‹åŒ– `wandb.init()` æ¥å¼€å§‹è®°å½•æ—¥å¿—ã€‚
   - ç¡®ä¿åœ¨ `wandb.init()` ä¸­æä¾› `project` å’Œ `entity` å‚æ•°ï¼Œåˆ†åˆ«è¡¨ç¤ºé¡¹ç›®åç§°å’Œä½ çš„ WandB ç”¨æˆ·åã€‚
   
2. **æ—¥å¿—è®°å½•**ï¼š
   - åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡ `wandb.log()` å°†å…³é”®æ•°æ®ï¼ˆå¦‚ç”¨æˆ·è¾“å…¥ã€ç³»ç»Ÿå“åº”ã€ä¼šè¯å†å²ç­‰ï¼‰è®°å½•ä¸‹æ¥ã€‚
   - `wandb.log()` æ˜¯ç”¨æ¥è®°å½•æ¯ä¸€æ­¥çš„æ—¥å¿—æ•°æ®ï¼Œå¯ä»¥è®°å½•æ–‡æœ¬ã€æ•°å­—ã€å›¾åƒç­‰å¤šç§ç±»å‹çš„æ•°æ®ã€‚

3. **ç›‘æ§æ¨¡å‹**ï¼š
   - é€šè¿‡åœ¨æ¯æ¬¡æ¨ç†æ—¶è®°å½•è¾“å…¥ã€è¾“å‡ºã€å¯¹è¯è½®æ¬¡ç­‰ä¿¡æ¯ï¼Œå¯ä»¥åœ¨ **WandB Dashboard** ä¸Šå®æ—¶ç›‘æ§æ¨¡å‹çš„æ€§èƒ½ã€‚
   - ä¾‹å¦‚ï¼šè§‚å¯Ÿä¸åŒè¾“å…¥çš„å“åº”æ—¶é—´ã€å†å²å¯¹è¯é•¿åº¦ä¸æ¨¡å‹ç”Ÿæˆçš„å“åº”è´¨é‡ã€‚

### ğŸ§© ä½¿ç”¨ WandB ç›‘æ§è®­ç»ƒ

- **ç›‘æ§å›¾è¡¨**ï¼šä½ å¯ä»¥åœ¨ **WandB Dashboard** ä¸­çœ‹åˆ°æ¯æ¬¡è®°å½•çš„å›¾è¡¨ï¼ŒåŒ…æ‹¬ï¼š
  - ç”¨æˆ·è¾“å…¥çš„åˆ†å¸ƒæƒ…å†µ
  - æ¨¡å‹ç”Ÿæˆçš„å“åº”åˆ†å¸ƒ
  - å…¶ä»–æŒ‡æ ‡ï¼ˆå¦‚ä¼šè¯è½®æ¬¡ï¼‰

---

### ğŸ§ª å¯åŠ¨ FastAPI æœåŠ¡å¹¶å¯ç”¨ WandB ç›‘æ§

```bash
uvicorn main:app --host 0.0.0.0 --port 8000
```

### ğŸ“Š WandB Dashboard æŸ¥çœ‹

- ç™»é™† [WandB](https://wandb.ai/) åï¼Œå¯ä»¥è¿›å…¥ **é¡¹ç›®é¡µé¢**ï¼ŒæŸ¥çœ‹æ—¥å¿—ä¿¡æ¯å¹¶åˆ›å»ºå›¾è¡¨ã€‚
- ä½ å°†èƒ½å¤Ÿçœ‹åˆ°æ—¥å¿—ä¸­çš„ **ç”¨æˆ·è¾“å…¥** å’Œ **ç³»ç»Ÿå“åº”**ï¼Œå¹¶èƒ½å¤Ÿé€šè¿‡å›¾è¡¨åˆ†ææ¨¡å‹è¡Œä¸ºã€‚

---

### ç»“è¯­

é€šè¿‡é›†æˆ **WandB**ï¼Œä½ å¯ä»¥å®æ—¶ç›‘æ§ FastAPI æœåŠ¡ä¸­æ¯æ¬¡æ¨ç†çš„è¾“å…¥ã€è¾“å‡ºä»¥åŠæ¨¡å‹çš„çŠ¶æ€ï¼Œä»è€Œæœ‰åŠ©äºè°ƒä¼˜å’Œæ”¹è¿›æ¨¡å‹çš„è¡¨ç°ã€‚



å¾®è°ƒ ChatGLM3 å¹¶å°†å…¶éƒ¨ç½²ä¸ºä¸€ä¸ªå¯ä¾›å®æ—¶ä½¿ç”¨çš„æœåŠ¡æ˜¯ä¸€ä¸ªå¤æ‚çš„æµç¨‹ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬å°†é€æ­¥å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

1. **å¾®è°ƒ ChatGLM3**ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ LoRAï¼ˆLow-Rank Adaptationï¼‰å¾®è°ƒ ChatGLM3ï¼Œä»¥ä¾¿åœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸Šæœ‰æ•ˆè°ƒæ•´å…¶æ€§èƒ½ã€‚
2. **éƒ¨ç½²å¾®è°ƒç‰ˆæœ¬**ï¼šåœ¨å¾®è°ƒå®Œæˆåï¼Œæˆ‘ä»¬å°†éƒ¨ç½²è¿™ä¸ªå¾®è°ƒçš„æ¨¡å‹ä¸ºä¸€ä¸ª **FastAPI** æœåŠ¡ï¼Œæ”¯æŒå®æ—¶æ¨ç†è¯·æ±‚ã€‚

### 1. å¾®è°ƒ ChatGLM3

é¦–å…ˆç¡®ä¿ä½ å·²ç»å‡†å¤‡å¥½äº†åˆé€‚çš„è®¡ç®—èµ„æºï¼ˆå¦‚å¤šGPUç¯å¢ƒï¼‰æ¥è¿›è¡Œå¾®è°ƒã€‚å¦‚æœä½ è¿˜æ²¡æœ‰å®‰è£… ChatGLM3 å’Œå…¶ä»–ä¾èµ–ï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œå®‰è£…ã€‚

#### å®‰è£…ä¾èµ–

```bash
pip install deepspeed transformers datasets accelerate torch wandb
pip install git+https://github.com/THU-KEG/ChatGLM-6B.git
pip install peft
```

#### å¾®è°ƒä»£ç ï¼šLoRA å¾®è°ƒ ChatGLM3

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import get_peft_model, LoraConfig, TaskType
from datasets import load_dataset
from tqdm import tqdm
from transformers import Trainer, TrainingArguments
import wandb

# é…ç½®WandB
wandb.init(project="chatglm3-finetuning", entity="your-wandb-username")

# åŠ è½½ChatGLM3æ¨¡å‹å’ŒTokenizer
model_name = "THU-KEG/chatglm-6b"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# é…ç½®LoRAå¾®è°ƒ
lora_config = LoraConfig(
    r=8, 
    lora_alpha=16,
    lora_dropout=0.1,
    task_type=TaskType.CAUSAL_LM,
)
model = get_peft_model(model, lora_config)

# åŠ è½½æ•°æ®é›†ï¼ˆä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†æˆ–MRPCï¼‰
dataset = load_dataset("glue", "mrpc", split="train")

# æ•°æ®é¢„å¤„ç†
def preprocess_function(examples):
    return tokenizer(examples['sentence1'], examples['sentence2'], padding=True, truncation=True)

encoded_dataset = dataset.map(preprocess_function, batched=True)

# é…ç½®è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./chatglm3-finetuned",
    evaluation_strategy="epoch",  # æ¯ä¸ªepochè¯„ä¼°ä¸€æ¬¡
    logging_dir="./logs",
    logging_steps=100,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    save_strategy="epoch",  # ä¿å­˜æ¨¡å‹ç­–ç•¥
    fp16=True,  # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    deepspeed="./deepspeed_config.json",  # DeepSpeedé…ç½®æ–‡ä»¶
)

# ä½¿ç”¨ Trainer API è®­ç»ƒæ¨¡å‹
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset,
    eval_dataset=encoded_dataset,
    tokenizer=tokenizer,
)

# å¯åŠ¨è®­ç»ƒ
trainer.train()
```

#### è®­ç»ƒé…ç½®æ–‡ä»¶ï¼š`deepspeed_config.json`

```json
{
  "train_batch_size": 16,
  "gradient_accumulation_steps": 2,
  "fp16": {
    "enabled": true
  },
  "zero_optimization": {
    "stage": 2
  },
  "offload_optimizer": {
    "device": "cpu",
    "pin_memory": true
  }
}
```

### 2. éƒ¨ç½²å¾®è°ƒåçš„ ChatGLM3 æ¨¡å‹

å¾®è°ƒå®Œæˆåï¼Œä¿å­˜æ¨¡å‹å¹¶å‡†å¤‡éƒ¨ç½²ï¼š

```python
# ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹
model.save_pretrained("./chatglm3-finetuned")
tokenizer.save_pretrained("./chatglm3-finetuned")
```

#### åˆ›å»º FastAPI æœåŠ¡

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å°†å¾®è°ƒåçš„æ¨¡å‹éƒ¨ç½²ä¸ºä¸€ä¸ª FastAPI æœåŠ¡ï¼Œæ”¯æŒå®æ—¶æ¨ç†ã€‚

```python
from fastapi import FastAPI, WebSocket
from transformers import AutoTokenizer, AutoModelForCausalLM
from pydantic import BaseModel
import torch

# åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("./chatglm3-finetuned")
tokenizer = AutoTokenizer.from_pretrained("./chatglm3-finetuned")

# FastAPIåº”ç”¨å®ä¾‹
app = FastAPI()

class ChatRequest(BaseModel):
    user_id: str
    session_id: str
    query: str

# æ¨ç†å‡½æ•°
def chat_with_model(query: str):
    inputs = tokenizer(query, return_tensors="pt").to("cuda")
    output = model.generate(inputs["input_ids"], max_length=512)
    return tokenizer.decode(output[0], skip_special_tokens=True)

@app.websocket("/ws/chat/{user_id}/{session_id}")
async def websocket_endpoint(websocket: WebSocket, user_id: str, session_id: str):
    await websocket.accept()
    try:
        while True:
            data = await websocket.receive_text()
            response = chat_with_model(data)
            await websocket.send_text(response)
    except WebSocketDisconnect:
        print(f"Client {user_id} disconnected")

@app.post("/chat")
async def chat(req: ChatRequest):
    response = chat_with_model(req.query)
    return {"response": response}
```

#### å¯åŠ¨ FastAPI æœåŠ¡

åœ¨ç»ˆç«¯ä¸­å¯åŠ¨ FastAPI åº”ç”¨ï¼š

```bash
uvicorn main:app --host 0.0.0.0 --port 8000
```

### 3. æ·»åŠ æ¨¡å‹ç›‘æ§ï¼ˆWandBï¼‰

åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬å·²ç»åˆå§‹åŒ–äº† WandB å¹¶åœ¨å¾®è°ƒæœŸé—´è®°å½•äº†æ¯ä¸ªæ¨ç†æ­¥éª¤çš„æ•°æ®ã€‚åœ¨ FastAPI æœåŠ¡ä¸­ï¼Œä½ ä¹Ÿå¯ä»¥è®°å½•å®æ—¶çš„æ¨ç†æ—¥å¿—ï¼Œå¦‚ç”¨æˆ·è¾“å…¥å’Œæ¨¡å‹è¾“å‡ºã€‚

```python
import wandb

# é…ç½®WandB
wandb.init(project="chatglm3-chat-service", entity="your-wandb-username")

# åœ¨æ¨ç†è¿‡ç¨‹ä¸­è®°å½•WandBæ—¥å¿—
def chat_with_model(query: str):
    inputs = tokenizer(query, return_tensors="pt").to("cuda")
    output = model.generate(inputs["input_ids"], max_length=512)
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    
    # è®°å½•æ—¥å¿—åˆ°WandB
    wandb.log({
        "user_input": query,
        "system_response": response
    })
    
    return response
```

### 4. å®Œæ•´çš„å¾®è°ƒä¸éƒ¨ç½²æµç¨‹

æ€»ç»“ä¸€ä¸‹ï¼Œå®Œæ•´çš„æµç¨‹åŒ…æ‹¬ï¼š

1. **æ¨¡å‹å¾®è°ƒ**ï¼šä½¿ç”¨ LoRA å¾®è°ƒ ChatGLM3ï¼Œå¹¶ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹ã€‚
2. **FastAPI æœåŠ¡**ï¼šå°†å¾®è°ƒåçš„æ¨¡å‹éƒ¨ç½²ä¸º FastAPI æœåŠ¡ï¼Œæ”¯æŒå®æ—¶ WebSocket æ¨ç†ã€‚
3. **æ—¥å¿—ç›‘æ§**ï¼šé€šè¿‡ WandB è®°å½•æ¨ç†è¿‡ç¨‹ä¸­çš„æ—¥å¿—ï¼Œä¾¿äºç›‘æ§å’Œåˆ†æã€‚

### 5. åç»­ä¼˜åŒ–

- **æ€§èƒ½ä¼˜åŒ–**ï¼šå¯ä»¥æ ¹æ®å®é™…è´Ÿè½½è°ƒæ•´ `batch_size` å’Œ `gradient_accumulation_steps`ã€‚
- **å¤šæœºå¤šå¡è®­ç»ƒ**ï¼šä½¿ç”¨ DeepSpeed æˆ–å…¶ä»–åˆ†å¸ƒå¼æ¡†æ¶æ‰©å±•è®­ç»ƒï¼Œæ”¯æŒå¤šGPUç¯å¢ƒã€‚
- **è´Ÿè½½å‡è¡¡**ï¼šå¯ä»¥ä½¿ç”¨åå‘ä»£ç†å¦‚ Nginx æˆ–å…¶ä»–è´Ÿè½½å‡è¡¡å™¨æ¥å¤„ç†å¤§è§„æ¨¡å¹¶å‘è¯·æ±‚ã€‚

---

è¿™æ ·ï¼Œä½ å°±å®Œæˆäº†ä»å¾®è°ƒ ChatGLM3 åˆ°éƒ¨ç½²ä¸º FastAPI æœåŠ¡å¹¶åŠ ä¸Šç›‘æ§çš„å…¨è¿‡ç¨‹ã€‚å¦‚æœä½ å¸Œæœ›è¿›ä¸€æ­¥æ‰©å±•åŠŸèƒ½ï¼Œå¦‚é›†æˆ Redis æŒä¹…åŒ–å†å²è®°å½•æˆ–å®šæ—¶ä»»åŠ¡ç­‰ï¼Œä¹Ÿå¯ä»¥ç»§ç»­åœ¨ç°æœ‰åŸºç¡€ä¸Šè¿›è¡Œä¼˜åŒ–ã€‚




ä¸ºäº†åœ¨ ChatGLM3 å¾®è°ƒå’Œ FastAPI æœåŠ¡ä¸­é›†æˆç”¨æˆ·ä¸Šä¸‹æ–‡è®°å¿†å¹¶å°†å…¶å­˜å…¥ MySQLï¼Œä½ å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œæ‰©å±•ã€‚æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªæ¨¡å‹æ¥å­˜å‚¨ç”¨æˆ·ä¸Šä¸‹æ–‡ï¼Œå¹¶åœ¨æ¯æ¬¡å¯¹è¯æ—¶æ›´æ–°å’ŒæŸ¥è¯¢ MySQL æ•°æ®åº“ã€‚

### 1. è®¾ç½® MySQL æ•°æ®åº“

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦è®¾ç½® MySQL æ•°æ®åº“ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªç”¨äºå­˜å‚¨ç”¨æˆ·å¯¹è¯å†å²çš„è¡¨ã€‚

#### åˆ›å»º MySQL æ•°æ®åº“å’Œè¡¨

```sql
CREATE DATABASE chat_memory;

USE chat_memory;

CREATE TABLE user_context (
    id INT AUTO_INCREMENT PRIMARY KEY,
    user_id VARCHAR(255) NOT NULL,
    session_id VARCHAR(255) NOT NULL,
    conversation TEXT NOT NULL,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### 2. å®‰è£… MySQL ä¾èµ–

åœ¨ FastAPI é¡¹ç›®ä¸­ä½¿ç”¨ MySQLï¼Œé¦–å…ˆä½ éœ€è¦å®‰è£…ç›¸å…³çš„ä¾èµ–åº“ï¼Œå¦‚ `mysql-connector-python` å’Œ `SQLAlchemy`ã€‚

```bash
pip install mysql-connector-python sqlalchemy
```

### 3. é…ç½®æ•°æ®åº“è¿æ¥

æˆ‘ä»¬å°†ä½¿ç”¨ SQLAlchemy æ¥ä¸ MySQL è¿›è¡Œäº¤äº’ã€‚ä»¥ä¸‹æ˜¯åˆ›å»ºæ•°æ®åº“è¿æ¥çš„é…ç½®ã€‚

#### åˆ›å»º `db.py` é…ç½®æ•°æ®åº“è¿æ¥

```python
from sqlalchemy import create_engine, Column, Integer, String, Text, TIMESTAMP
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
import mysql.connector

# MySQLæ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²
DATABASE_URL = "mysql+mysqlconnector://username:password@localhost/chat_memory"

# åˆ›å»ºSQLAlchemyåŸºç¡€ç±»
Base = declarative_base()

# å®šä¹‰UserContextæ¨¡å‹
class UserContext(Base):
    __tablename__ = "user_context"
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    user_id = Column(String(255), nullable=False)
    session_id = Column(String(255), nullable=False)
    conversation = Column(Text, nullable=False)
    timestamp = Column(TIMESTAMP, default="CURRENT_TIMESTAMP")

# åˆ›å»ºæ•°æ®åº“å¼•æ“
engine = create_engine(DATABASE_URL)

# åˆ›å»ºæ•°æ®åº“ä¼šè¯
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# åˆ›å»ºæ•°æ®åº“è¡¨
Base.metadata.create_all(bind=engine)

# è·å–ä¼šè¯
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

### 4. ä¿®æ”¹ FastAPI æœåŠ¡ä»¥æ”¯æŒç”¨æˆ·ä¸Šä¸‹æ–‡

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ‰©å±• FastAPI æœåŠ¡ï¼Œå°†ç”¨æˆ·çš„å¯¹è¯ä¸Šä¸‹æ–‡ä¿å­˜åˆ° MySQL ä¸­ï¼Œå¹¶åœ¨éœ€è¦æ—¶æ£€ç´¢å’Œæ›´æ–°å®ƒã€‚

#### æ›´æ–° FastAPI æœåŠ¡ï¼šä¿å­˜ä¸æ£€ç´¢ç”¨æˆ·å¯¹è¯ä¸Šä¸‹æ–‡

```python
from fastapi import FastAPI, WebSocket, Depends, HTTPException
from transformers import AutoTokenizer, AutoModelForCausalLM
from pydantic import BaseModel
from sqlalchemy.orm import Session
from db import get_db, UserContext
import torch
import wandb

# åˆå§‹åŒ– FastAPI å’Œ WandB
app = FastAPI()

# åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("./chatglm3-finetuned")
tokenizer = AutoTokenizer.from_pretrained("./chatglm3-finetuned")

# é…ç½®WandB
wandb.init(project="chatglm3-chat-service", entity="your-wandb-username")

# æ¨ç†å‡½æ•°
def chat_with_model(query: str):
    inputs = tokenizer(query, return_tensors="pt").to("cuda")
    output = model.generate(inputs["input_ids"], max_length=512)
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    
    # è®°å½•æ—¥å¿—åˆ°WandB
    wandb.log({
        "user_input": query,
        "system_response": response
    })
    
    return response

# è·å–æˆ–åˆ›å»ºç”¨æˆ·ä¸Šä¸‹æ–‡
def get_user_context(db: Session, user_id: str, session_id: str):
    context = db.query(UserContext).filter(UserContext.user_id == user_id, UserContext.session_id == session_id).first()
    if not context:
        context = UserContext(user_id=user_id, session_id=session_id, conversation="")
        db.add(context)
        db.commit()
    return context

# æ›´æ–°ç”¨æˆ·ä¸Šä¸‹æ–‡
def update_user_context(db: Session, user_id: str, session_id: str, new_conversation: str):
    context = db.query(UserContext).filter(UserContext.user_id == user_id, UserContext.session_id == session_id).first()
    if context:
        context.conversation = new_conversation
        db.commit()

# WebSocket è¿æ¥å¤„ç†ï¼šæ”¯æŒç”¨æˆ·ä¸Šä¸‹æ–‡
@app.websocket("/ws/chat/{user_id}/{session_id}")
async def websocket_endpoint(websocket: WebSocket, user_id: str, session_id: str, db: Session = Depends(get_db)):
    await websocket.accept()
    
    # è·å–æˆ–åˆ›å»ºç”¨æˆ·ä¸Šä¸‹æ–‡
    user_context = get_user_context(db, user_id, session_id)
    
    try:
        while True:
            data = await websocket.receive_text()
            
            # è·å–å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡
            conversation_history = user_context.conversation
            full_query = conversation_history + "\nUser: " + data + "\nAI:"

            # æ‰§è¡Œæ¨ç†
            response = chat_with_model(full_query)

            # æ›´æ–°ç”¨æˆ·ä¸Šä¸‹æ–‡
            updated_conversation = conversation_history + "\nUser: " + data + "\nAI:" + response
            update_user_context(db, user_id, session_id, updated_conversation)

            # å‘é€å“åº”
            await websocket.send_text(response)

    except WebSocketDisconnect:
        print(f"Client {user_id} disconnected")

# HTTP è¯·æ±‚å¤„ç†ï¼šæ”¯æŒç”¨æˆ·ä¸Šä¸‹æ–‡
class ChatRequest(BaseModel):
    user_id: str
    session_id: str
    query: str

@app.post("/chat")
async def chat(req: ChatRequest, db: Session = Depends(get_db)):
    user_context = get_user_context(db, req.user_id, req.session_id)
    
    # è·å–å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡
    conversation_history = user_context.conversation
    full_query = conversation_history + "\nUser: " + req.query + "\nAI:"

    # æ‰§è¡Œæ¨ç†
    response = chat_with_model(full_query)

    # æ›´æ–°ç”¨æˆ·ä¸Šä¸‹æ–‡
    updated_conversation = conversation_history + "\nUser: " + req.query + "\nAI:" + response
    update_user_context(db, req.user_id, req.session_id, updated_conversation)

    return {"response": response}
```

### 5. å¯åŠ¨æœåŠ¡

1. å¯åŠ¨ FastAPI æœåŠ¡ï¼š

```bash
uvicorn main:app --host 0.0.0.0 --port 8000
```

2. è®¿é—® `http://localhost:8000/docs` æŸ¥çœ‹ FastAPI è‡ªåŠ¨ç”Ÿæˆçš„æ–‡æ¡£ç•Œé¢ï¼Œå¹¶æµ‹è¯• `/chat` æ¥å£ã€‚

### 6. ä»£ç è§£é‡Š

1. **æ•°æ®åº“è¿æ¥**ï¼šé€šè¿‡ SQLAlchemy è¿æ¥ MySQL æ•°æ®åº“ï¼Œå¹¶å®šä¹‰äº† `UserContext` è¡¨æ¥å­˜å‚¨ç”¨æˆ·çš„å¯¹è¯ä¸Šä¸‹æ–‡ã€‚æ¯ä¸ªç”¨æˆ·çš„ `user_id` å’Œ `session_id` ä¼šå”¯ä¸€å¯¹åº”ä¸€æ¡è®°å½•ï¼Œå­˜å‚¨ç”¨æˆ·ä¸ AI çš„å¯¹è¯ã€‚
2. **WebSocket æ”¯æŒ**ï¼šå½“ç”¨æˆ·ä¸æœåŠ¡å™¨é€šè¿‡ WebSocket è¿æ¥æ—¶ï¼ŒèŠå¤©è®°å½•ä¼šåœ¨ MySQL ä¸­æ›´æ–°ï¼Œæ¯ä¸ªæ–°çš„æ¶ˆæ¯éƒ½ä¼šè¢«é™„åŠ åˆ°ç”¨æˆ·çš„å¯¹è¯è®°å½•ä¸­ã€‚
3. **ä¸Šä¸‹æ–‡æ›´æ–°**ï¼šæ¯æ¬¡æ”¶åˆ°ç”¨æˆ·çš„æ¶ˆæ¯ï¼Œç³»ç»Ÿä¼šç»“åˆç°æœ‰çš„å¯¹è¯å†å²è¿›è¡Œæ¨ç†ï¼Œå¹¶å°†æœ€æ–°çš„å¯¹è¯è®°å½•æ›´æ–°å›æ•°æ®åº“ã€‚
4. **WandB æ—¥å¿—**ï¼šé€šè¿‡ `wandb.log()` å°†ç”¨æˆ·è¾“å…¥å’Œç³»ç»Ÿè¾“å‡ºè¿›è¡Œæ—¥å¿—è®°å½•ï¼Œä¾¿äºç›‘æ§ã€‚

### 7. åç»­ä¼˜åŒ–

- **å®šæœŸæ¸…ç†æ—§è®°å½•**ï¼šå¯ä»¥è€ƒè™‘ä¸ºç”¨æˆ·å¯¹è¯å†å²æ·»åŠ è¿‡æœŸæ—¶é—´ï¼Œä¾‹å¦‚æ¯éš”ä¸€æ®µæ—¶é—´æ¸…ç†ä¸å†æ´»è·ƒç”¨æˆ·çš„å†å²è®°å½•ï¼Œæˆ–è€…æ ¹æ®ä¸šåŠ¡éœ€æ±‚å®šæœŸæ¸…ç†å†å²ã€‚
- **æ€§èƒ½ä¼˜åŒ–**ï¼šå½“å¯¹è¯å†å²å˜å¾—å¾ˆé•¿æ—¶ï¼Œå¯ä»¥è€ƒè™‘å¼•å…¥ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ç­–ç•¥ï¼Œé¿å…è¶…è¿‡æ¨¡å‹çš„æœ€å¤§è¾“å…¥é•¿åº¦ã€‚
- **é«˜å¯ç”¨æ€§éƒ¨ç½²**ï¼šå¦‚æœéœ€è¦éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼Œè€ƒè™‘å°† FastAPI æœåŠ¡ä¸ Docker å®¹å™¨ã€Kubernetes ç­‰æŠ€æœ¯ç»“åˆï¼Œç¡®ä¿é«˜å¯ç”¨æ€§å’Œæ°´å¹³æ‰©å±•ã€‚

é€šè¿‡è¿™ç§æ–¹å¼ï¼Œä½ å¯ä»¥ä¸ºæ¯ä¸ªç”¨æˆ·ç»´æŠ¤ä¸€ä¸ªé•¿æœŸå¯¹è¯ä¸Šä¸‹æ–‡ï¼Œå¹¶å°†è¿™äº›æ•°æ®å­˜å‚¨åœ¨ MySQL ä¸­ï¼Œä»è€Œæä¾›æŒç»­çš„ã€ä¸ªæ€§åŒ–çš„å¯¹è¯æœåŠ¡ã€‚


å½“å¯¹è¯å†å²å˜å¾—å¾ˆé•¿æ—¶ï¼Œè¶…è¿‡æ¨¡å‹çš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆä¾‹å¦‚ï¼Œå¯¹äº GPT-2ã€ChatGLM æˆ–å…¶ä»–å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šå¸¸æœ‰ 512ã€1024 æˆ– 2048 token çš„é™åˆ¶ï¼‰å¯èƒ½ä¼šå¯¼è‡´æ¨ç†å¤±è´¥æˆ–æ€§èƒ½ä¸‹é™ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å¼•å…¥ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ç­–ç•¥ï¼Œå¹¶å®šæœŸæ¸…ç†æ—§è®°å½•ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†åˆç†é•¿åº¦çš„å¯¹è¯è¾“å…¥ã€‚

### 1. ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ç­–ç•¥

ä¸ºäº†ç¡®ä¿æ¯æ¬¡æ¨ç†æ—¶ï¼Œè¾“å…¥çš„å¯¹è¯å†å²ä¸ä¼šè¶…è¿‡æ¨¡å‹çš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼Œæˆ‘ä»¬éœ€è¦åœ¨æ¯æ¬¡æ›´æ–°å¯¹è¯å†å²æ—¶è¿›è¡Œæ§åˆ¶ã€‚ä¸‹é¢ä»‹ç»ä¸¤ç§å¸¸è§çš„ç­–ç•¥ï¼š

- **æˆªæ–­å†å²**ï¼šåœ¨å¯¹è¯å†å²è¶…è¿‡æœ€å¤§é•¿åº¦æ—¶ï¼Œæˆªæ–­æ‰è¾ƒæ—§çš„éƒ¨åˆ†ï¼Œåªä¿ç•™æœ€æ–°çš„å¯¹è¯è®°å½•ã€‚
- **ç²¾ç®€å†å²**ï¼šå¯¹å†å²è¿›è¡Œç­›é€‰ï¼Œåˆ é™¤æ— å…³çš„éƒ¨åˆ†ï¼Œåªä¿ç•™é‡è¦çš„å¯¹è¯ä¿¡æ¯ã€‚

### 2. å®ç°ä¸Šä¸‹æ–‡é•¿åº¦æ§åˆ¶

å‡è®¾æˆ‘ä»¬ä½¿ç”¨ ChatGLM æˆ–ç±»ä¼¼çš„æ¨¡å‹ï¼Œæœ€å¤§è¾“å…¥é•¿åº¦ä¸º 2048 tokensã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `tokenizer.encode` æˆ– `tokenizer.batch_encode_plus` è·å–è¾“å…¥æ–‡æœ¬çš„ token æ•°é‡ï¼Œå¹¶æ ¹æ®æœ€å¤§é•¿åº¦è¿›è¡Œè°ƒæ•´ã€‚

#### ä¿®æ”¹ `chat_with_model` å‡½æ•°å®ç°ä¸Šä¸‹æ–‡é•¿åº¦æ§åˆ¶

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# åŠ è½½å¾®è°ƒåçš„æ¨¡å‹å’Œtokenizer
model = AutoModelForCausalLM.from_pretrained("./chatglm3-finetuned")
tokenizer = AutoTokenizer.from_pretrained("./chatglm3-finetuned")

MAX_INPUT_LENGTH = 2048  # å‡è®¾æœ€å¤§è¾“å…¥é•¿åº¦ä¸º2048 tokens

# æ¨ç†å‡½æ•°ï¼Œæ·»åŠ ä¸Šä¸‹æ–‡é•¿åº¦æ§åˆ¶
def chat_with_model(query: str, conversation_history: str):
    # åˆå¹¶å†å²å’Œå½“å‰ç”¨æˆ·è¾“å…¥
    full_query = conversation_history + "\nUser: " + query + "\nAI:"

    # ç¼–ç æ–‡æœ¬å¹¶æ£€æŸ¥é•¿åº¦
    input_ids = tokenizer.encode(full_query, return_tensors="pt").to("cuda")
    
    # å¦‚æœè¾“å…¥çš„tokenæ•°è¶…è¿‡æœ€å¤§é™åˆ¶ï¼Œæˆªæ–­æ—§çš„éƒ¨åˆ†
    if input_ids.shape[1] > MAX_INPUT_LENGTH:
        # è®¡ç®—éœ€è¦æˆªæ–­çš„é•¿åº¦
        truncated_input_ids = input_ids[:, -MAX_INPUT_LENGTH:]
        # è§£ç å¹¶ç”Ÿæˆå›å¤
        output = model.generate(truncated_input_ids, max_length=512)
    else:
        # å¦‚æœæ²¡æœ‰è¶…å‡ºæœ€å¤§é•¿åº¦ï¼Œç›´æ¥ç”Ÿæˆ
        output = model.generate(input_ids, max_length=512)

    response = tokenizer.decode(output[0], skip_special_tokens=True)
    return response
```

### 3. å®šæœŸæ¸…ç†æ—§è®°å½•

ä¸ºäº†é¿å…æ•°æ®åº“ä¸­è¿‡äºåºå¤§çš„å¯¹è¯å†å²æ•°æ®å ç”¨è¿‡å¤šå­˜å‚¨ç©ºé—´ï¼Œæˆ‘ä»¬å¯ä»¥å®šæœŸæ¸…ç†ä¸å†æ´»è·ƒçš„ç”¨æˆ·å¯¹è¯è®°å½•ã€‚å¯ä»¥é€šè¿‡è®¾ç½®è¿‡æœŸæ—¶é—´ï¼ˆå¦‚æ¯ 30 å¤©æ¸…ç†ä¸€æ¬¡æ—§çš„å¯¹è¯è®°å½•ï¼‰æˆ–åŸºäºç”¨æˆ·æ´»è·ƒåº¦è¿›è¡Œæ¸…ç†ã€‚

#### æ·»åŠ æ¸…ç†å†å²çš„åŠŸèƒ½

æˆ‘ä»¬å¯ä»¥åœ¨æ•°æ®åº“ä¸­å¢åŠ ä¸€ä¸ª `timestamp` å­—æ®µï¼Œå¹¶æ ¹æ®æ—¶é—´æˆ³æ¥å®šæœŸæ¸…ç†è¿‡æœŸçš„è®°å½•ã€‚

```python
from sqlalchemy import func

# å®šæœŸæ¸…ç†è¿‡æœŸè®°å½•
def clean_old_conversations(db: Session, expiration_days: int = 30):
    # è·å–å½“å‰æ—¶é—´å‡å»è¿‡æœŸå¤©æ•°
    expiration_time = func.now() - func.interval(f"{expiration_days} day")
    
    # åˆ é™¤è¿‡æœŸçš„è®°å½•
    db.query(UserContext).filter(UserContext.timestamp < expiration_time).delete()
    db.commit()
```

#### å®šæ—¶ä»»åŠ¡

ä½ å¯ä»¥ä½¿ç”¨å¦‚ `APScheduler` æˆ– `Celery` ç­‰å·¥å…·æ¥å®šæœŸæ¸…ç†è¿‡æœŸçš„ç”¨æˆ·å¯¹è¯è®°å½•ã€‚

##### ä½¿ç”¨ APScheduler å®šæœŸæ¸…ç†ä»»åŠ¡

é¦–å…ˆï¼Œå®‰è£… `APScheduler`ï¼š

```bash
pip install apscheduler
```

ç„¶åï¼Œåœ¨ FastAPI æœåŠ¡ä¸­è®¾ç½®å®šæ—¶æ¸…ç†ä»»åŠ¡ï¼š

```python
from apscheduler.schedulers.background import BackgroundScheduler
from datetime import datetime

# å®šæ—¶ä»»åŠ¡ï¼šæ¯å¤©åˆå¤œæ¸…ç†è¿‡æœŸè®°å½•
def scheduled_clean():
    with SessionLocal() as db:
        clean_old_conversations(db, expiration_days=30)

# å¯åŠ¨è°ƒåº¦å™¨
scheduler = BackgroundScheduler()
scheduler.add_job(scheduled_clean, 'interval', days=1, next_run_time=datetime.now())
scheduler.start()

# å¯åŠ¨ FastAPI åº”ç”¨æ—¶ï¼Œå¯åŠ¨è°ƒåº¦å™¨
@app.on_event("startup")
async def startup():
    scheduler.start()
```

### 4. æ›´æ–°ä¸Šä¸‹æ–‡ç®¡ç†ç­–ç•¥

ç»“åˆä¸Šè¿°ä¸Šä¸‹æ–‡æ§åˆ¶å’Œå®šæœŸæ¸…ç†è®°å½•ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿æ¯æ¬¡ç”Ÿæˆå¯¹è¯æ—¶è¾“å…¥çš„ä¸Šä¸‹æ–‡ä¸ä¼šè¿‡é•¿ï¼ŒåŒæ—¶ä¹Ÿä¿æŒæ•°æ®åº“ä¸­è®°å½•çš„åˆç†æ€§ã€‚

#### æ›´æ–°å¯¹è¯å†å²ç®¡ç†

æˆ‘ä»¬æ›´æ–°ç”¨æˆ·å¯¹è¯å†å²æ—¶ï¼Œä¼šå¯¹å†å²å†…å®¹è¿›è¡Œæ£€æŸ¥å’Œæ›´æ–°ï¼Œä¿è¯ä¸è¶…è¿‡æœ€å¤§é•¿åº¦é™åˆ¶ã€‚

```python
# æ›´æ–°ç”¨æˆ·ä¸Šä¸‹æ–‡
def update_user_context(db: Session, user_id: str, session_id: str, new_conversation: str):
    context = db.query(UserContext).filter(UserContext.user_id == user_id, UserContext.session_id == session_id).first()
    
    if context:
        # æ£€æŸ¥å†å²å¯¹è¯çš„é•¿åº¦ï¼Œè¿›è¡Œå¿…è¦çš„æˆªæ–­
        conversation_history = context.conversation + "\n" + new_conversation
        # æˆªæ–­è¶…å‡ºæœ€å¤§é•¿åº¦çš„éƒ¨åˆ†
        input_ids = tokenizer.encode(conversation_history, return_tensors="pt").to("cuda")
        if input_ids.shape[1] > MAX_INPUT_LENGTH:
            truncated_conversation = tokenizer.decode(input_ids[:, -MAX_INPUT_LENGTH:], skip_special_tokens=True)
        else:
            truncated_conversation = conversation_history
        context.conversation = truncated_conversation
        db.commit()
```

### 5. æ€»ç»“

é€šè¿‡ä¸Šè¿°ç­–ç•¥å’Œå®ç°ï¼š

- æˆ‘ä»¬å¯ä»¥åœ¨æ¯æ¬¡æ¨ç†æ—¶åŠ¨æ€æ§åˆ¶è¾“å…¥çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œé¿å…è¶…è¿‡æ¨¡å‹çš„æœ€å¤§è¾“å…¥é™åˆ¶ã€‚
- æˆ‘ä»¬å®šæœŸæ¸…ç†è¿‡æœŸçš„å¯¹è¯å†å²ï¼Œä»¥å‡å°‘å­˜å‚¨å‹åŠ›ã€‚
- ä½¿ç”¨å®šæ—¶ä»»åŠ¡ï¼ˆå¦‚ `APScheduler`ï¼‰å®šæœŸæ‰§è¡Œæ¸…ç†ä»»åŠ¡ï¼Œç¡®ä¿æ•°æ®åº“ä¸­ä¿æŒåˆç†å¤§å°çš„å¯¹è¯è®°å½•ã€‚

è¿™äº›æªæ–½èƒ½æœ‰æ•ˆå¸®åŠ©ä½ åœ¨é•¿æ—¶é—´è¿è¡Œå¯¹è¯æœåŠ¡æ—¶ï¼Œç¡®ä¿æœåŠ¡çš„é«˜æ•ˆæ€§å’Œç¨³å®šæ€§ï¼Œé¿å…å‡ºç° `OOM` æˆ–æ€§èƒ½ä¸‹é™ç­‰é—®é¢˜ã€‚



åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œç¡®ä¿æœåŠ¡çš„é«˜å¯ç”¨æ€§å’Œå¯æ‰©å±•æ€§è‡³å…³é‡è¦ã€‚ç»“åˆ Docker å®¹å™¨å’Œ Kubernetes å¯ä»¥å¸®åŠ©å®ç°è¿™äº›ç›®æ ‡ã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•å°† FastAPI æœåŠ¡ä¸ Docker å’Œ Kubernetes æŠ€æœ¯ç»“åˆï¼Œä»¥ç¡®ä¿é«˜å¯ç”¨æ€§å’Œæ°´å¹³æ‰©å±•çš„åŸºæœ¬æ­¥éª¤ã€‚

### 1. Docker åŒ– FastAPI æœåŠ¡

#### æ­¥éª¤ 1ï¼šåˆ›å»º Dockerfile

é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ª `Dockerfile`ï¼Œå®šä¹‰å¦‚ä½•æ„å»º FastAPI æœåŠ¡çš„é•œåƒã€‚å‡è®¾ä½ å·²ç»æœ‰ä¸€ä¸ª FastAPI é¡¹ç›®å¹¶ä¸”éœ€è¦å°†å…¶éƒ¨ç½²ä¸ºå®¹å™¨ã€‚

```Dockerfile
# ä½¿ç”¨å®˜æ–¹ Python é•œåƒä½œä¸ºåŸºç¡€é•œåƒ
FROM python:3.9-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶ä¾èµ–é¡¹æ¸…å•æ–‡ä»¶åˆ°å®¹å™¨ä¸­
COPY requirements.txt .

# å®‰è£… FastAPI å’Œç›¸å…³ä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶æ•´ä¸ªé¡¹ç›®åˆ°å®¹å™¨ä¸­
COPY . /app

# è®¾ç½® FastAPI è¿è¡Œæ—¶å‘½ä»¤
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

åœ¨ `requirements.txt` ä¸­æ·»åŠ ä½ æ‰€éœ€è¦çš„ä¾èµ–é¡¹ï¼Œä¾‹å¦‚ï¼š

```txt
fastapi
uvicorn
redis
sqlalchemy
```

#### æ­¥éª¤ 2ï¼šæ„å»º Docker é•œåƒ

åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œæ„å»º Docker é•œåƒï¼š

```bash
docker build -t my-fastapi-app .
```

#### æ­¥éª¤ 3ï¼šè¿è¡Œ Docker å®¹å™¨

æ„å»ºå®Œæˆåï¼Œä½ å¯ä»¥è¿è¡Œå®¹å™¨ï¼š

```bash
docker run -d -p 8000:8000 my-fastapi-app
```

è¿™å°†åœ¨æœ¬åœ°çš„ `8000` ç«¯å£å¯åŠ¨ FastAPI æœåŠ¡ã€‚

### 2. å°† FastAPI æœåŠ¡éƒ¨ç½²åˆ° Kubernetes

Kubernetes æ˜¯ä¸€ç§å¼€æºå¹³å°ï¼Œè‡ªåŠ¨åŒ–äº†åº”ç”¨ç¨‹åºçš„éƒ¨ç½²ã€æ‰©å±•å’Œç®¡ç†ã€‚ä¸‹é¢ä»‹ç»å¦‚ä½•ä½¿ç”¨ Kubernetes éƒ¨ç½²å’Œç®¡ç†ä½ çš„ FastAPI æœåŠ¡ã€‚

#### æ­¥éª¤ 1ï¼šåˆ›å»º Kubernetes éƒ¨ç½²é…ç½®æ–‡ä»¶

é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ªåä¸º `deployment.yaml` çš„ Kubernetes é…ç½®æ–‡ä»¶ï¼Œæè¿°å¦‚ä½•éƒ¨ç½² FastAPI æœåŠ¡ã€‚

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fastapi-deployment
spec:
  replicas: 3  # éƒ¨ç½²3ä¸ªå‰¯æœ¬ä»¥å®ç°é«˜å¯ç”¨æ€§
  selector:
    matchLabels:
      app: fastapi
  template:
    metadata:
      labels:
        app: fastapi
    spec:
      containers:
        - name: fastapi
          image: my-fastapi-app:latest  # è¿™é‡Œä½¿ç”¨ä¹‹å‰æ„å»ºçš„é•œåƒ
          ports:
            - containerPort: 8000
---
apiVersion: v1
kind: Service
metadata:
  name: fastapi-service
spec:
  selector:
    app: fastapi
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8000
  type: LoadBalancer  # è¿™é‡Œä½¿ç”¨ LoadBalancer ç±»å‹ï¼Œæ”¯æŒè‡ªåŠ¨åˆ†é…å¤–éƒ¨ IP
```

- **replicas**ï¼šæŒ‡å®šè¦è¿è¡Œçš„å‰¯æœ¬æ•°ï¼ˆè¿™é‡Œæ˜¯3ï¼‰ï¼Œå¯ä»¥æ ¹æ®å®é™…è´Ÿè½½è¿›è¡Œè°ƒæ•´ã€‚
- **selector**ï¼šåŒ¹é…æ ‡è®°ä¸º `app: fastapi` çš„æ‰€æœ‰å®¹å™¨ã€‚
- **Service**ï¼šå®šä¹‰ä¸€ä¸ª Kubernetes Serviceï¼Œç”¨äºæš´éœ² FastAPI æœåŠ¡ï¼Œæ–¹ä¾¿å¤–éƒ¨è®¿é—®ã€‚

#### æ­¥éª¤ 2ï¼šåº”ç”¨ Kubernetes é…ç½®

ä½¿ç”¨ `kubectl` å‘½ä»¤å°†ä¸Šè¿°é…ç½®åº”ç”¨åˆ° Kubernetes é›†ç¾¤ä¸­ã€‚

```bash
kubectl apply -f deployment.yaml
```

#### æ­¥éª¤ 3ï¼šæ£€æŸ¥éƒ¨ç½²çŠ¶æ€

ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ£€æŸ¥éƒ¨ç½²çŠ¶æ€ï¼š

```bash
kubectl get deployments
kubectl get pods
kubectl get svc
```

ç¡®ä¿ `fastapi-deployment` éƒ¨ç½²æˆåŠŸï¼Œå¹¶ä¸”è‡³å°‘æœ‰ä¸€ä¸ª `Pod` æ­£åœ¨è¿è¡Œã€‚

#### æ­¥éª¤ 4ï¼šè®¿é—®æœåŠ¡

å¦‚æœä½ ä½¿ç”¨çš„æ˜¯ `LoadBalancer` ç±»å‹çš„æœåŠ¡ï¼Œä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è·å–å¤–éƒ¨ IP åœ°å€ï¼š

```bash
kubectl get svc fastapi-service
```

å¦‚æœä½¿ç”¨çš„æ˜¯é›†ç¾¤å†…éƒ¨è´Ÿè½½å‡è¡¡å™¨ï¼Œä½ å¯ä»¥ä½¿ç”¨é›†ç¾¤å†…éƒ¨çš„ DNS åç§°æ¥è®¿é—®æœåŠ¡ã€‚

### 3. é«˜å¯ç”¨æ€§å’Œæ°´å¹³æ‰©å±•

Kubernetes èƒ½å¤Ÿè‡ªåŠ¨æ‰©å±•å’Œè´Ÿè½½å‡è¡¡æµé‡ï¼Œä»¥ç¡®ä¿é«˜å¯ç”¨æ€§ã€‚

#### æ­¥éª¤ 1ï¼šæ°´å¹³è‡ªåŠ¨æ‰©å±• (Horizontal Pod Autoscaler)

Kubernetes æ”¯æŒæ ¹æ® CPU æˆ–å†…å­˜çš„ä½¿ç”¨æƒ…å†µè‡ªåŠ¨æ‰©å±•åº”ç”¨ç¨‹åºçš„å‰¯æœ¬æ•°ã€‚ä½ å¯ä»¥åˆ›å»ºä¸€ä¸ªè‡ªåŠ¨æ‰©å±•å™¨ï¼ˆHPAï¼‰ï¼Œå®ƒæ ¹æ®è´Ÿè½½æƒ…å†µè‡ªåŠ¨å¢åŠ æˆ–å‡å°‘ `Pod` æ•°é‡ã€‚

é¦–å…ˆï¼Œç¡®ä¿ä½ å·²å¯ç”¨ `metrics-server`ï¼Œå®ƒæ˜¯ Kubernetes æä¾›çš„ç”¨äºèµ„æºä½¿ç”¨æŒ‡æ ‡çš„æœåŠ¡ã€‚ç„¶åï¼Œåˆ›å»ºä¸€ä¸ª HPA é…ç½®ï¼š

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: fastapi-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: fastapi-deployment
  minReplicas: 3
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
```

è¿™ä¸ªé…ç½®æ–‡ä»¶è®¾ç½®äº†ï¼š

- `minReplicas`ï¼šæœ€å°å‰¯æœ¬æ•°ã€‚
- `maxReplicas`ï¼šæœ€å¤§å‰¯æœ¬æ•°ã€‚
- `averageUtilization`ï¼šCPU ä½¿ç”¨ç‡è¾¾åˆ° 80% æ—¶ï¼Œè‡ªåŠ¨æ‰©å±•å‰¯æœ¬æ•°ã€‚

ç„¶åï¼Œåº”ç”¨ HPA é…ç½®ï¼š

```bash
kubectl apply -f hpa.yaml
```

#### æ­¥éª¤ 2ï¼šè´Ÿè½½å‡è¡¡

Kubernetes ä¸­çš„ `Service` å¯¹è±¡ä¼šè‡ªåŠ¨ä¸ºå¤šä¸ªå‰¯æœ¬æä¾›è´Ÿè½½å‡è¡¡ã€‚å¦‚æœä½ ä½¿ç”¨äº† `LoadBalancer` ç±»å‹çš„æœåŠ¡ï¼Œå®ƒå°†ä¼šé€šè¿‡å¤–éƒ¨è´Ÿè½½å‡è¡¡å™¨ï¼ˆå¦‚äº‘æä¾›å•†çš„è´Ÿè½½å‡è¡¡å™¨ï¼‰åˆ†å‘æµé‡ã€‚

### 4. æŒç»­é›†æˆä¸æŒç»­äº¤ä»˜ï¼ˆCI/CDï¼‰

ä¸ºäº†å®ç°é«˜æ•ˆçš„å¼€å‘å’Œç”Ÿäº§ç¯å¢ƒå‘å¸ƒï¼Œä½ å¯ä»¥ä½¿ç”¨ CI/CD å·¥å…·ï¼ˆå¦‚ GitLab CIã€GitHub Actionsã€Jenkins ç­‰ï¼‰æ¥å®ç°æŒç»­é›†æˆå’ŒæŒç»­äº¤ä»˜ã€‚è¿™åŒ…æ‹¬ï¼š

- è‡ªåŠ¨æ„å»º Docker é•œåƒå¹¶æ¨é€åˆ°å®¹å™¨é•œåƒä»“åº“ï¼ˆå¦‚ Docker Hub æˆ– GitHub Packagesï¼‰ã€‚
- è‡ªåŠ¨éƒ¨ç½²æ›´æ–°åˆ° Kubernetes é›†ç¾¤ã€‚
- è‡ªåŠ¨åŒ–æµ‹è¯•å’ŒéªŒè¯ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„ GitHub Actions ç¤ºä¾‹é…ç½®ï¼š

```yaml
name: CI/CD for FastAPI app

on:
  push:
    branches:
      - main

jobs:
  build_and_deploy:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v2

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v1

    - name: Build and push Docker image
      uses: docker/build-push-action@v2
      with:
        context: .
        push: true
        tags: my-fastapi-app:latest

    - name: Deploy to Kubernetes
      run: |
        kubectl apply -f deployment.yaml
```

### 5. æ€»ç»“

é€šè¿‡ç»“åˆ Docker å’Œ Kubernetesï¼Œå¯ä»¥ç¡®ä¿ FastAPI æœåŠ¡å…·æœ‰é«˜å¯ç”¨æ€§ã€æ°´å¹³æ‰©å±•èƒ½åŠ›ï¼Œå¹¶ä¸”èƒ½å¤Ÿè‡ªåŠ¨å¤„ç†è´Ÿè½½å˜åŒ–ã€‚ä¸Šè¿°æµç¨‹åŒ…æ‹¬ï¼š

- **Docker**ï¼šå°† FastAPI æœåŠ¡å®¹å™¨åŒ–ï¼Œæ–¹ä¾¿éƒ¨ç½²ã€‚
- **Kubernetes**ï¼šé€šè¿‡éƒ¨ç½²ã€æœåŠ¡ã€è´Ÿè½½å‡è¡¡å’Œè‡ªåŠ¨æ‰©å±•å®ç°é«˜å¯ç”¨æ€§ã€‚
- **æ°´å¹³æ‰©å±•**ï¼šä½¿ç”¨ Kubernetes Horizontal Pod Autoscaler (HPA) åŠ¨æ€æ‰©å±•å‰¯æœ¬æ•°ã€‚
- **CI/CD**ï¼šè‡ªåŠ¨åŒ–æ„å»ºã€æµ‹è¯•å’Œéƒ¨ç½²æµç¨‹ï¼Œç¡®ä¿å¿«é€Ÿæ›´æ–°ã€‚

è¿™ç§éƒ¨ç½²æ–¹å¼ä½¿å¾—ä½ çš„ FastAPI åº”ç”¨å…·å¤‡äº†é«˜å¯ç”¨æ€§ã€å¯æ‰©å±•æ€§ä»¥åŠçµæ´»çš„ç®¡ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿé€‚åº”ç”Ÿäº§ç¯å¢ƒä¸­çš„å¤§è§„æ¨¡è´Ÿè½½ã€‚
















