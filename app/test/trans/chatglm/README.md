å¤ªå¥½äº†ï¼Œæˆ‘ä»¬ç°åœ¨è¿›å…¥å®æˆ˜çš„ä¸­é«˜çº§é˜¶æ®µï¼šä½¿ç”¨ **ChatGLM2-6B + LoRA å¾®è°ƒ** è¿›è¡Œä¸­æ–‡ä»»åŠ¡å¾®è°ƒï¼

---

## ğŸš€ ä½¿ç”¨ ChatGLM2-6B + LoRAï¼ˆPEFTï¼‰è¿›è¡Œä¸­æ–‡æ¨¡å‹å¾®è°ƒ

### âœ… åº”ç”¨åœºæ™¯ï¼šç”¨è¾ƒå°èµ„æºå¾®è°ƒå¤§æ¨¡å‹ï¼ˆå¦‚ ChatGLM2-6Bï¼‰ï¼Œç”¨äºä¸­æ–‡é—®ç­”ã€æ‘˜è¦ã€æƒ…æ„Ÿåˆ†æç­‰ä»»åŠ¡ã€‚

---

## ä¸€ã€ç¯å¢ƒå‡†å¤‡ï¼ˆpip å®‰è£…ï¼‰

```bash
pip install transformers accelerate datasets peft bitsandbytes
```

---

## äºŒã€å®Œæ•´ä»£ç ç¤ºä¾‹ï¼ˆå«æ³¨é‡Šï¼‰  
ä¸‹é¢ä»¥ ChatGLM2-6B æ¨¡å‹å¾®è°ƒä¸­æ–‡é—®ç­”ä»»åŠ¡ä¸ºä¾‹ï¼Œä½¿ç”¨ `LoRA` å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ï¼Œä»…è°ƒæ•´å°é‡å‚æ•°ã€‚

### ğŸ“„ `finetune_chatglm2_lora.py`

```python
import torch
from transformers import AutoTokenizer, AutoModel
from peft import get_peft_model, LoraConfig, TaskType
from datasets import load_dataset
from torch.utils.data import DataLoader
from transformers import TrainingArguments, Trainer
from transformers import DataCollatorForSeq2Seq

# åŠ è½½ ChatGLM2 æ¨¡å‹å’Œåˆ†è¯å™¨ï¼ˆæ”¯æŒå¤šè¯­è¨€ï¼‰
model_name = "THUDM/chatglm2-6b"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModel.from_pretrained(model_name, trust_remote_code=True, device_map="auto")

# å¯ç”¨æ¨ç†æ¨¡å¼ï¼ˆèŠ‚çœå†…å­˜ï¼‰
model.eval()

# æ„å»º LoRA é…ç½®
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,  # å› æœè¯­è¨€å»ºæ¨¡
    r=8,                           # Rank å€¼ï¼Œæ§åˆ¶ LoRA å­ç©ºé—´çš„ç»´åº¦
    lora_alpha=32,                # æ”¾å¤§å› å­
    lora_dropout=0.1,             # Dropout é¿å…è¿‡æ‹Ÿåˆ
    bias="none",                  # ä¸å¯¹ bias åš LoRA
    target_modules=["query_key_value"]  # æŒ‡å®šæ³¨å…¥ LoRA çš„æ¨¡å—ï¼ˆChatGLM ç‰¹æœ‰æ¨¡å—åï¼‰
)

# æ³¨å…¥ LoRA åˆ°åŸå§‹æ¨¡å‹ä¸­
model = get_peft_model(model, peft_config)

# åŠ è½½ä¸€ä¸ªå°è§„æ¨¡ä¸­æ–‡é—®ç­”æ•°æ®é›†ï¼ˆä½ å¯ä»¥æ›¿æ¢ä¸º CMRCã€DuReader ç­‰ï¼‰
dataset = load_dataset("json", data_files={"train": "data/train.json", "test": "data/test.json"})

# æ•°æ®æ ¼å¼é¢„å¤„ç†å‡½æ•°ï¼šå°† "instruction" + "input" æ‹¼æ¥ä¸º promptï¼Œoutput ä½œä¸ºæ ‡ç­¾
def preprocess_function(example):
    prompt = example["instruction"] + "\n" + example["input"]
    inputs = tokenizer(prompt, truncation=True, padding="max_length", max_length=512)
    labels = tokenizer(example["output"], truncation=True, padding="max_length", max_length=512)
    inputs["labels"] = labels["input_ids"]
    return inputs

# åº”ç”¨é¢„å¤„ç†
tokenized_dataset = dataset.map(preprocess_function)

# æ„å»º DataLoader ç”¨äº Trainer
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

# è®¾ç½®è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./chatglm2-lora-output",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    num_train_epochs=2,
    logging_steps=10,
    save_strategy="epoch",
    learning_rate=2e-4,
    fp16=True,
    remove_unused_columns=False
)

# ä½¿ç”¨ HuggingFace Trainer å°è£…è®­ç»ƒæµç¨‹
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    tokenizer=tokenizer,
    data_collator=data_collator
)

# å¼€å§‹è®­ç»ƒ
trainer.train()
```

---

## ä¸‰ã€æ•°æ®æ ¼å¼è¦æ±‚ï¼ˆ`train.json` ç¤ºä¾‹ï¼‰

```json
[
  {
    "instruction": "è¯·å¯¹ä¸‹åˆ—å¥å­è¿›è¡Œæƒ…æ„Ÿåˆ†æï¼š",
    "input": "è¿™å®¶é¤å…çš„èœåˆè´µåˆéš¾åƒã€‚",
    "output": "è´Ÿé¢"
  },
  {
    "instruction": "è¯·æ€»ç»“ä»¥ä¸‹å†…å®¹çš„è¦ç‚¹ï¼š",
    "input": "è‹¹æœå…¬å¸å‘å¸ƒäº†æ–°ä¸€ä»£MacBook Proï¼Œé‡‡ç”¨M3èŠ¯ç‰‡...",
    "output": "è‹¹æœå‘å¸ƒæ–°MacBook Proï¼Œæ­è½½M3èŠ¯ç‰‡"
  }
]
```

---

## å››ã€è¯´æ˜

| éƒ¨åˆ†                 | ä½œç”¨ |
|----------------------|------|
| `LoRA`               | ä»…å¾®è°ƒéƒ¨åˆ†æ¨¡å—çš„ä½ç§©å­ç©ºé—´ï¼Œå‚æ•°é‡ä¸‹é™ 95%+ï¼Œè®­ç»ƒæ•ˆç‡å¤§å¹…æå‡ |
| `ChatGLM2-6B`        | THUDM å¼€æºçš„å¤§æ¨¡å‹ï¼Œæ”¯æŒä¸­è‹±å¯¹è¯ä»»åŠ¡ã€é—®ç­”ä»»åŠ¡ |
| `peft.get_peft_model`| ç”¨äºæ³¨å…¥ä½ç§©é€‚é…å™¨ï¼ˆLoRAï¼‰åˆ°ç›®æ ‡æ¨¡å— |
| `Trainer`            | HuggingFace å°è£…çš„è®­ç»ƒå™¨ï¼Œç®€åŒ–è®­ç»ƒæµç¨‹ |

---

## ğŸ§  æ¨èå®è·µ

- âœ… ä½¿ç”¨ä½ çš„ä¸­æ–‡ç§æœ‰è¯­æ–™æ•°æ®é›†
- âœ… å¼€å¯ `deepspeed` or `fsdp` è¿›ä¸€æ­¥é™ä½æ˜¾å­˜
- âœ… ä½¿ç”¨ wandb or TensorBoard ç›‘æ§æŒ‡æ ‡

---

å¦‚æœä½ æƒ³ç»§ç»­æˆ‘å¯ä»¥ä¸ºä½ æ‹“å±•ï¼š

âœ… åŠ å…¥ DeepSpeed/FSDP + PEFT  
âœ… å¤š GPU æ‰§è¡Œé…ç½®  
âœ… ä½¿ç”¨ ChatGLM3 / Baichuan2 / LLaMA3 ä¸­æ–‡å¾®è°ƒç‰ˆæœ¬  
âœ… å®ç°æ¨ç† APIã€é‡åŒ–ä¿å­˜æ¨¡å‹  

éœ€è¦æˆ‘ç»§ç»­å®Œå–„è¿™äº›å†…å®¹å—ï¼Ÿä½ æƒ³å¾®è°ƒå“ªä¸ªå…·ä½“æ¨¡å‹/ä»»åŠ¡ä¹Ÿå¯ä»¥ç›´æ¥è¯´ ğŸ˜